{"./":{"url":"./","title":"Introduction","keywords":"","body":"Aoang's Wiki 书山有路勤为径，学海无涯苦作舟。 人类自诞生以来，一直在进化，也一直在退化。 在这段强制性的假期中，我看的第一本是 鲁滨逊漂流记，其实这本书已经看过好几遍了。 但是看了看经年之前的读后感，其中的想法和现在还是有着惊人的相似。 如果是我，我能活下来吗？ 如果为此做准备，你需要学会什么？ 如果现在你已经到了那个地步，你能活多久？ 不知有没有人会钻木取火、提取海盐、制作工具等，看过相关的书籍也算。 如果看过，那现在还记得多少。 其实编程和在现实生活中生存是一样的，什么是一样的？逻辑性、哲学。 比如，一座海岛上有一片森林，有的长得茂密有的却一般，这是因为什么？这是因为风水不好。 代码没有得到预期的效果，这是因为什么？这是因为你蠢。 能影响到森林的因素有什么？阳光、水分、土壤、动物、未知因素。 能影响到代码运行结果的因素有什么？人、代码、未知因素。 Leetcode 有一道很经典的题目叫 两数之和，如果把这个题目转换成现实问题。 那应该是，给你一本书，书中的每一页都有一个数字，再给你一个目标值。 让你在书中找出和为目标值的那两个数字的页码。 两数之和很多人都会写，但是如果转换成现实中的问题之后，你遇到了，你会这么做吗？ 编码能力高、解耦能力强，却没有将其活学活用，这是不是一种悲哀。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"简言/":{"url":"简言/","title":"简言","keywords":"","body":"简言 书山有路勤为径，学海无涯苦作舟 过分的简单往往是一种负担 如果你知道自己在做什么，就去做吧，不要让规则束缚了你自己 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"golang/1.数据类型.html":{"url":"golang/1.数据类型.html","title":"数据类型","keywords":"","body":"数据类型 简介中提到 Go 是一个静态强类型语言，在常见的编程语言中，它们的数据类型都会以动静态、强弱来区分。当然也有特例，汇编是无类型的。 比如 C/C++ 是静态弱类型语言，Java C# 是静态强类型，还有动态弱类型：JavaScript PHP，动态强类型：Python Scheme 什么是动静态、强弱类型呢？ 对于动静态类型而言，在编译时就知道变量类型的，是静态类型；运行时才知道一个变量类型的，叫做动态类型。 对于 Go 语言，有时候定义变量不会声明类型，例如 i := 0。 这名为 类型推导，当定义一个变量却不显式指定类型时候，类型由值推导而出。 var i int j := i // j 是 int 类型 k := 0 // k 是 int 类型 如果值是数字，则新的变量可能是 int、float64、complex128，这时候类型推导会取决于常量的精度。 i := 42 // int f := 3.142 // float64 g := 0.867 + 0.5i // complex128 类型推导通过已知的类型在编译时期推导出不知道的变量的类型，在静态类型语言中对一个变量做该变量类型所不允许的操作会报出语法错误。 具有类型推论的语言除了 Go 还有：Rust Haskell Scala C# C++ 而强弱类型的区别在于是否允许隐式转换，允许的是弱类型，反之是强类型。 在 Python 中进行 '666' / 2 你会得到一个类型错误，这是因为强类型语言中是不允许隐式转换的。 而在 JavaScript 中进行 '666' / 2 你会得到整数 333，这是因为在执行运算的时候字符串 '666' 先被转换成整数 666，然后再进行除法运算。 强类型语言一般需要在运行时运行一套类型检查系统，因此强类型语言的速度一般比弱类型要慢。 弱类型语言由于在运行时缺乏类型系统，在初学 C 语言的时候就可能会遇到数组访问越界的情况。 静态类型由于在编译期会进行优化，所以一般来说性能是比较高的。 动态语言在进行类型操作的时候还需要解释器去猜测其类型，因此性能会低一些。 在上述所说的动静态、强弱类型的四种语言中，不考虑其他因素单就类型而言，执行的速度应该是 C > Java > JavaScript > Python 不过静态强类型的语言写起来往往更安全的。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"golang/1.1.基本数据类型.html":{"url":"golang/1.1.基本数据类型.html","title":"基本数据类型","keywords":"","body":"简单数据类型 数据类型的出现是为了把数据分成所需内存大小不同的数据，编程的时候需要用大数据的时候才需要申请大内存，就可以充分利用内存。 Go 语言基本数据类型有：布尔型、数字类型、字符串类型 布尔型 布尔型的值只可以是常量 true 或者 false 一个简单的例子：var b bool = true 名称 类型和描述 默认值 存储空间(bit) 值范围 bool boolean false 1 true 或 false 数字类型 整型 int 和浮点型 float32、float64，Go 语言支持整型和浮点型数字，并且支持复数，其中位的运算采用补码。 名称 类型和描述 默认值 存储空间(bit) 值范围 int 平台相关(int32/64) 0 平台相关 平台相关 uint 平台相关(uint32/64) 0 平台相关 平台相关 int8 8位有符号整型 0 8 -128 ~ 127 uint8 8位无符号整型 0 8 0 ~ 255 int16 16位有符号整型 0 16 -32768 ~ 32767 uint16 16位无符号整型 0 16 0 ~ 65535 int32 32位有符号整型 0 32 -2147483648 ~ 2147483647 uint32 32位无符号整型 0 32 0 ~ 4294967295 int64 64位有符号整型 0 64 -9223372036854775808 ~ 9223372036854775807 uint64 64位无符号整型 0 64 0 ~ 18446744073709551615 float32 32位浮点类型 0.0 32 IEEE-754 float64 64位浮点类型 0.0 64 IEEE-754 complex64 由float32实部+虚部 0+0i 64 对应 float32 complex128 由float64实部+虚部 0+0i 128 对应 float64 byte uint8的别名 0 8 UTF-8 单个字节的值 rune int32的别名 0 32 单个 Unicode 字符 uintptr 用于一个存放指针 0 平台相关 平台相关 整数 十进制整数，使用 0-9 的数字表示且不以 0 开头。 八进制整数，以 0 开头，0-7 的数字表示 十六进制整数，以 0X 或者是 0x 开头，0-9|A-F|a-f 组成 浮点数由整数部分、小数点和小数部分组成，整数部分和小数部分可以隐藏其中一种。也可以使用科学计数法表示: 0. 72.40 072.40 和 72.40 一样 2.71823 1.e+0 6.67428e-11 1E6 .25 .12345E+5 复数的虚部,由一个整数或者是一个小数加上i表示 0i 011i //==11i 这里部分八进制了 0.i 2.71825i 1.e+0i 6.23423-11i 1E6i .25i .1234E+5i byte 对应 ASCII 码的字符值 rune 表示 Unicode 的字符 字符串类型: 字符串就是一串固定长度的字符连接起来的字符序列。Go 的字符串是由单个字节连接起来的。 Go 语言的字符串的字节使用 UTF-8 编码标识 Unicode 文本。 Go 语言支持以下 2 种形式的字符串： str ：= \"laoYu\" // 解释性字符串,该类字符串使用双引号括起来，其中的相关的转义字符将被替换 `This is a raw string \\n` // 原生字符串该类字符串使用反引号括起来，支持换行 string 是不可改变的，一旦创建，string 的内容就不能被改变。 a := “test” a[1] = ‘a’ // 不能改变 string 的值 &a[i] // 不能对 string 的元素进行取地址 a[i] // 访问的是第i个 byte len(i)是 byte 的个数，unicode可能占据多个 byte for , ch := range a { // 可以遍历string的每一个unicode } for , uni := range []rune(a) { // 可以遍历string的每一个unicode } for _, ch := range []byte(a) { //可以遍历string的每一个 byte } 字符串底层设计type stringStruct struct { str unsafe.Pointer // 指向一个 [len]byte 的数组 len int // 长度 } 点击查看 字符串底层设计 str 为字符数组，len 为数组长度。Go 的字符串是不可变类型，对 string 类型的变量初始化意味着会对底层结构的初始化。 至于为什么str用byte类型而不用rune类型，Go 的 for 循环对字符串的遍历是基于字节的，所以 str 用 byte 类型而不用 rune 类型。 当然，如果有必要，可以转成 rune 切片或使用 range 来迭代。 内建函数 len 对 string 类型的操作是直接从底层结构中取出 len 值，不需要额外的操作，当然在初始化时必需同时初始化 len 的值。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"golang/1.2.数组.html":{"url":"golang/1.2.数组.html","title":"数组","keywords":"","body":"数组 作为一种新兴的语言，如果仅仅是为了某种特定的用途，那么可能其内置类型不是很多，仅需要能够完成基本功能即可。 但是 Go 不仅支持几乎所有语言都支持的简单数据类型（比如整型和浮点型等）外，还支持一些其他的高级类型， 比如 map 类型，要知道这些类型在其他语言中都是通过包的形式引入的外部数据类型。 slice 类似于 C++ STL 中的 vector，在 Go 中也是一种内置的数据类型作为动态数组来使用。 既然绝大多数开发者都需要用到这个类型，为什么还非要每个人都写一行 import 语句来包含一个库？ 当然，可能有人会提到 Go 没有泛型等等，只要是必要的功能，相信后期都会加上去的。 相较于其他编程语言，Go 语言对于某些方面可能算是非常好的了。 例如，Java 生态不错，但是基本上多数都是使用的老版本 1.6 1.8 这样的，并不会一味追求更新的版本。 不过说回来，可能是因为追求新特性的用户都去 Scala 了吧。 C++ 标准太臃肿了，Go 的诞生就有一部分原因是如此，而且风格、库都是各用各的。 言归正传，来看看 Go 中的数组。 声明、初始化 声明 3 个 int 型变量的数组 var arr [3]int arr[0] = 123 arr[1] = 110 arr[2] = 234 定义数组时为数组元素赋值 var arr [10]int = [10]int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10} 为部分元素赋值 var arr [10]int = [10]int{1, 2, 3, 4, 5} 指定下标赋值 var arr [10]int = [10]int{1: 10, 4: 20, 6: 30} 根据元素个数赋值并创建数组 var arr [4]int = [...]int{1, 2, 3, 4} var 写多了的确挺烦的 这种声明方式已经很贴近 slice 了，不过还是留到后面的篇幅在做记录吧 arr := [3]int{1, 22, 333} arr := [...]int{1, 2, 3, 4} arr := [5]int{1:1,3:4} 多维数组 var arr [3][4]int arr := [3][4]int{ {0, 1, 2, 3}, {4, 5, 6, 7}, {8, 9, 10, 11}, } 访问修改 访问单个元素 arr := [6]int{100,200,300,400,500,600} fmt.Printfln(arr[0]) for-range 常见的遍历访问方式 arr := [6]int{100,200,300,400,500,600} for k,v := range arr{ fmt.Printf(\"arr[%d]=%d\\n\", k, v) } for 循环访问 arr := [6]int{100,200,300,400,500,600} for i := 0; i 修改元素 arr := [5]int{1: 1, 3: 4} arr[1] = 3 同类型的数组可以相互赋值 长度一样，并且每个元素的类型一样的数组，才是同样类型的数组。 array := [5]int{1: 1, 3: 4} var array1 [5]int = array // success var array2 [4]int = array1 // error 指针数组 指针数组和数组本身差不多，只不过元素类型是指针。 创建方式和普通数组不同，需要分配空间，数组元素的默认值为 nil，访问方式和普通数组差不多加上 * 即可。 arr := [5]*int{1: new(int), 3:new(int)} *arr[1] = 1 如果你现在给 arr[0] 赋值，会报 painc，原因已经写在上面了，解决办法如下： arr[0] = new(int) *arr[0] = 2 基本概念 其实前面已经提到了一部分概念，不过还有一些没提到，这里一起总结下来。 数组是 元素类型相同 且 长度固定 的存储在内存中数据类型 元素类型相同 且 长度固定 的数组才是同样类型的数组 数组是值类型。意思就是如果你将一个数组传递给另一个数组，那么，实际上就是整个数组拷贝了一份，函数传递也是如此 数组占用连续的内存，索引访问的效率非常高 数组指针和指针数组 x, y := 1, 2 var arr = [...]int{5: 2} // 数组指针 var pf *[6]int = &arr // 指针数组 pfArr := [...]*int{&x, &y} 数组指针是一个指向数组地址的一个指针 指针数组是一个数组内的元素全是指针类型 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"golang/1.3.切片.html":{"url":"golang/1.3.切片.html","title":"切片","keywords":"","body":"数组 切片其实和数组区别不大，你可以把它当成是动态数组来看待。它可以按需自动改变大小，使用这种结构，可以更方便的管理和使用数据集合。 声明、初始化 切片和数组在声明上的区别在于没有指定固定长度 slice := []int{1, 22, 333} slice := []int{1, 2, 3, 4} slice := []int{1: 1, 3: 4} 空切片与 nil 切片 它们的长度和容量都是 0，但是它们指向底层数组的指针不一样。 nil 切片意味着指向底层数组的指针为 nil，表示不存在的切片； 空切片对应的指针是个地址，空切片表示一个空集合。 var slice []int // nil 切片 ( slice == nil ) slice := []int{} // 空切片 // 分配内存 slice = make([]int, 10) // 直接声明并分配内存 slice := make([]int, 10) slice[0] = 123 slice[1] = 110 slice[2] = 234 声明具有容量的切片slice := make([]int, 5, 10) 使用内置的 make 函数时，需要传入一个参数，指定切片的长度，如果不指定容量，那么切片的容量和长度将会一样。 可以指定切片的容量，这个容量对应的是切片底层数组的。 因为切片的底层是数组，所以创建切片时，如果不指定字面值的话，默认值就是数组的元素的零值。 如果指定了容量是 10，但是这里只能访问 5 个元素，因为切片的长度是 5，剩下的 5 个元素，需要切片扩充后才可以访问。 容量必须大于等于长度，不能创建长度大于容量的切片的。 基于现有的数组或者切片创建 slice := []int{1, 2, 3, 4, 5} slice1 := slice[:] slice2 := slice[0:] slice3 := slice[:5] 切片会共用底层数组 slice := []int{1, 2, 3, 4, 5} newSlice := slice[1:3] newSlice[0] = 10 fmt.Println(slice) fmt.Println(newSlice) 切片扩容 切片算是一个动态数组，可以使用 append 函数扩容，append 函数可以为一个切片追加一个元素。 至于如何增加、返回的是原切片还是一个新切片、长度和容量如何改变这些细节，append 函数都会自动处理。 内置的 append 是一个可变参数的函数，可以同时追加多个值。 slice := []int{1, 2, 3, 4, 5} newSlice := slice[1:3] newSlice=append(newSlice,10) fmt.Println(newSlice) fmt.Println(slice) 如果切片的底层数组没有足够的容量，会新建一个底层数组，把原来数组的值复制到新底层数组里，再追加新值，这时不会影响原来的底层数组。 所以一般在创建新切片的时候保持长度和容量一样，这样在 append 的时候就不会因为共用底层数组而引起其他的问题。 当然，特殊需求特殊对待，如果你知道自己在做什么，那就去做吧，不需要理会这些。 扩容策略：容量小于 1024 个时直接翻倍；容量超过 1024 时，扩容为 1.25 倍。 切片 copy 这里的 copy 不是引用传递，切片起始指针会改变 s1 := []int{1, 2,3, 4, 5} s2 := make([]int, 5) copy(s2, s1) fmt.Printf(\"%p\\n\", s1) fmt.Printf(\"%p\\n\", s2) 基本概念 数组和切片在使用时其实息息相关、也很密切，概念上，只会和数组做一下区分 声明slice时候，方括号内为空 传递切片时传递的不是副本、而是指针 其本身并不是数组，它指向底层数组 作为变长数组的替代方案，可以关联底层数组的局部或者全部 可以直接创建或从底层数组生成 一般使用make创建 如果多个切片指向相同的底层数组，其中一个值改变会影响全部 meke([]T, len, cap) cap 省略时和 len 相同 len() 获取元素个数，cap() 获取容量 底层设计 type slice struct { array unsafe.Pointer // 指向数组的指针 len int // 长度 cap int // 容量 } 点击查看 切片底层设计 array 为底层数组，len 为实际存放的个数，cap 为总容量。使用 make 对 slice 进行初始化，也可以类似于数组的方式进行初始化。 当使用 make 函数来对 slice 进行初始化时，第一个参数为切片类型，第二个参数为 len，第三个参数可选，如果不传入，则cap等于len。 通常传入 cap 参数来预先分配大小的 slice，避免频繁重新分配内存。 切片是基于数组实现的，它的底层是数组，它自己本身非常小，可以理解为对底层数组的抽象。 因为基于数组实现，所以它的底层的内存是连续分配的，效率非常高，还可以通过索引获得数据，可以迭代以及垃圾回收优化的好处。 切片与字符串 前面有提到 string 的底层设计是一个 byte 的数组，有也可以进行切片操作。 str := \"hello world\" s1 := str[0:5] fmt.Println(s1) powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"golang/1.4.map.html":{"url":"golang/1.4.map.html","title":"Map","keywords":"","body":"Map Map 是由键值对组成的无序集合，也称为字典。 Map 是基于散列表来实现的，就是 Hash 表，每次迭代 Map 的时候，打印的 Key 和 Value 都是无序的。 其实某个版本之前是有序的，后来给改了 hhh... 声明、初始化 Map 需要分配内存，不然不能使用 var dict map[string]int // 声明，但未分配内存 dict = make(map[string]int) // 分配内存 dict[\"s\"] = 43 dict := make(map[string]int) dict[\"s\"] = 43 dict := map[string]int{\"s\":43} 访问与使用 在 Go 中，是可以获取一个不存在的键的值的，只不过返回的是值类型的零值。 检查一个键对应的值是否存在s, exists := dict[\"s\"] if exists { fmt.Println(s) } 返回的参数中，第一个是键对应的值，第二个是一个 bool 类型变量，对应这个键值对是否存在 使用 delete 函数删除键值对delete(dict, \"s\") delete 函数接受两个参数，第一个是要操作的 Map，第二个是要删除的 Map 的键。 delete函数删除不存在的键也是可以的...... Map 在传递的时候是引用传递，意思就是传递的不是副本。 并发 Map 不是线程安全的，但 sync.Map 是线程安全的，如果懒得自己实现，就直接用 sync.Map 好了。 底层设计 const ( // 一个 bucket 最多能放的元素数 bucketCntBits = 3 bucketCnt = 1 点击查看 Map 底层设计 在map的应用场景中选择 hash 函数主要考察性能、碰撞概率。 Go 使用的 hash 算法根据硬件选择，如果 CPU 支持 AES，那么使用 AES，否则使用 memhash。 哈希函数会将传入的key值进行哈希运算，得到一个唯一的值。 Go 把生成的哈希值一分为二，比如一个哈希值为：8423452987653321 拆分为 84234529 87653321。 前半部分就叫做高位哈希值，后半部分就叫做低位哈希值。 高位哈希值：用来确定当前的 bucket 有没有所存储的数据的 低位哈希值：用来确定当前的数据存在了哪个 bucket hmap 是 map 的最外层的一个数据结构，包括了 map 的各种基础信息，如大小、bucket。 buckets 这个参数，它存储的是指向 buckets 数组的一个指针，当 bucket(桶为0时)为nil。 可以理解为 hmap 指向了一个空 bucket 数组，并且当 bucket 数组需要扩容时，它会开辟一倍的内存空间，并且会渐进式的把原数组拷贝 即用到旧数组的时候就拷贝到新数组。 bucket 这三部分内容决定了它是怎么工作的： tophash 存储的是哈希函数算出的哈希值的高八位用来加快索引。 因为把高八位存储起来，这样不用完整比较 key 就能过滤掉不符合的 key，加快查询速度。 当一个哈希值的高8位和存储的高8位相符合，再去比较完整的key值，进而取出value。 第二部分，存储的是key 和value，就是传入的key和value，注意，它的底层排列方式是，key全部放在一起，value全部放在一起。 当key大于128字节时，bucket的key字段存储的会是指针，指向key的实际内容；value也是一样。 这样排列好处是在key和value的长度不同的时候，可以消除padding带来的空间浪费。并且每个bucket最多存放8个键值对。 第三部分，存储的是当bucket溢出时，指向的下一个bucket的指针。 bucket的设计细节： 在 map 中出现冲突时，不是每一个key都申请一个结构通过链表串起来，而是以bmap为最小粒度挂载，一个bmap可以放8个key和value。 这样减少对象数量，减轻管理内存的负担，利于gc。 如果插入时，bmap中key超过8，那么就会申请一个新的bmap（overflow bucket）挂在这个bmap的后面形成链表，优先用预分配的overflow bucket 如果预分配的用完了，那么就malloc一个挂上去。注意golang的map不会shrink，内存只会越用越多，overflow bucket中的key全删了也不会释放 简单概念 hmap存储了一个指向底层bucket数组的指针。 我们存入的key和value是存储在bucket里面中，如果key和value大于128字节，那么bucket里面存储的是指向我们key和value的指针，如果不是则存储的是值。 每个bucket 存储8个key和value，如果超过就重新创建一个bucket挂在在元bucket上，持续挂接形成链表。 高位哈希值：是用来确定当前的bucket（桶）有没有所存储的数据的。 低位哈希值：是用来确定，当前的数据存在了哪个bucket（桶） 基本流程 查找或者操作map时，首先key经过hash函数生成hash值，通过哈希值的低8位来判断当前数据属于哪个桶(bucket) 找到bucket以后，通过哈希值的高八位与bucket存储的高位哈希值循环比对，如果相同就比较刚才找到的底层数组的key值，如果key相同，取出value。 如果高八位hash值在此bucket没有，或者有，但是key不相同，就去链表中下一个溢出bucket中查找，直到查找到链表的末尾。 碰撞冲突：如果不同的key定位到了统一bucket或者生成了同一hash,就产生冲突。 go是通过链表法来解决冲突的。 比如一个高八位的hash值和已经存入的hash值相同，并且此bucket存的8个键值对已经满了，或者后面已经挂了好几个bucket了。 那么这时候要存这个值就先比对key,key肯定不相同啊，那就从此位置一直沿着链表往后找，找到一个空位置，存入它。 所以这种情况，两个相同的hash值高8位是存在不同bucket中的。 查的时候也是比对hash值和key 沿着链表把它查出来。 还有一种情况，就是目前就 1个bucket，并且8个key-value的数组还没有存满， 这个时候再比较完key不相同的时候，同样是沿着当前bucket数组中的内存空间往后找，找到第一个空位，插入它。 这个就相当于是用寻址法来解决冲突，查找的时候，也是先比较hash值，再比较key,然后沿着当前内存地址往后找。 go语言的map通过数组+链表的方式实现了hash表，同时分散各个桶，使用链表法+bucket内部的寻址法解决了碰撞冲突，也提高了效率。 因为即使链表很长了，go会根据装载因子，去扩容整个bucket数组，所以下面就要看下扩容。 扩容 当链表越来越长，bucket的扩容次数达到一定值，其实是bmap扩容的加载因数达到6.5，bmap就会进行扩容，将原来bucket数组数量扩充一倍， 产生一个新的bucket数组，也就是bmap的buckets属性指向的数组。这样bmap中的oldbuckets属性指向的就是旧bucket数组。 这里的加载因子LoadFactor是一个阈值，计算方式为（map长度/2^B ）如果超过6.5，将会进行扩容，这个是经过测试才得出的合理的一个阈值。 因为，加载因子越小，空间利用率就小，加载因子越大，产生冲突的几率就大。所以6.5是一个平衡的值。 map的扩容不会立马全部复制，而是渐进式扩容，即首先开辟2倍的内存空间，创建一个新的bucket数组。只有当访问原来就的bucket数组时， 才会将就得bucket拷贝到新的bucket数组，进行渐进式的扩容。当然旧的数据不会删除，而是去掉引用，等待gc回收。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"golang/1.5.结构体.html":{"url":"golang/1.5.结构体.html","title":"结构体","keywords":"","body":"结构体 和 C 语言一样，Go 也有 struct，与数组一样属于复合类型，并非引用类型，与其他面向对象编程语言中的类一样可以定义字段（属性）和方法，但也有很不同的地方。 定义 使用 struct 关键字可以定义一个结构体，结构体中的成员称为结构体的字段或属性。 type Member struct { ID int Name string Email string Gender int } 当然你也可以这么定义。 type Member struct { ID int Name, Email string Gender int } 结构体可以不包含任何字段，称为空结构体，struct{} 表示一个空的结构体，不过直接定义一个空的结构体并没有什么意义。 但在 channel 之间的通讯中可以用一个空结构体作为信号量。 ch := make(chan struct{}) ch 使用 直接定义变量，不会为字段赋初始值，所有字段都会赋默认值。 var m Member 使用字面量创建变量 m := Member { 1, \"小明\", \"xiaoming@email.com\", 1, } m := Member { id: 2, \"name\": \"小红\", } 访问字段 fmt.Println(m.ID) m.Name = \"小花\" 指针结构体 结构体与数组一样都是值传递，比如当把数组或结构体作为实参传给函数的形参时，会复制一个副本。 所以为了提高性能，一般把结构体传给函数时使用指针结构体。 func main() { m := NewMember(\"小明\") fmt.Println(m.name) } func NewMember(name string) *Member { return &Member{ \"name\": name, } } 可见性 在 Go 语言中，无论是结构体还是结构体内的字段又或是函数、常变量，它们的可见性都是由首字母的大小写决定的。 package member // 结构体对外可见 type Member struct { ID int // 字段对外可见 Name string // 字段对外可见 Email string // 字段对外可见 gender int // 字段对外不可见 } // 结构体对外可见 type member struct { ID int // 字段对外不可见 Name string // 字段对外不可见 Email string // 字段对外不可见 gender int // 字段对外不可见 } 两个一样的结构体，就因为结构体名不同，就会影响它们的可见性。 在代码中可以看到，如果一个结构体对外不可见，那么就算它的字段是大写的也不会对外可见。 值得一提的是，Go 语言支持（UTF-8 编码）中文变量、函数名，但是它们都是不可见的（中文是小写系列）。 变量 := 123 fmt.Println(变量) Tags 在定义结构体字段时，可以使用反引号为结构体字段声明元信息，用于编译阶段关联到字段当中。 type Member struct { ID int `json:\"id\"` Name string `json:\"name\"` Email string `json:\"email\"` Gender int `json:\"gender\"` } tag 的使用是函数定义的，上面定义的信息是标准库 encoding/json 编解码使用的。 type Member struct { ID int `json:\"id, -\"` Name string `json:\"name\"` Email string `json:\"email\"` Gender int `json:\"gender\"` } 这些信息不仅是一个字符串那么简单，因为其主要用于反射场景，reflect 包中提供了操作方法，所以写法也要遵循一定的规则。 Tag 本身是一个字符串，但字符串中却是：以空格分隔的 key:value 对。 key: 必须是非空字符串，字符串不能包含控制字符、空格、引号、冒号。 value: 以双引号标记的字符串 冒号前后不能有空格 type Server struct { ServerName string `key1: \"value1\" key11:\"value11\"` ServerIP string `key2: \"value2\"` } tag 的用途很广泛，常见一点就是 json 编解码，orm 框架等等。 基本概念 结构体是复合类型，无论是作为实参传递给函数时，还是赋值给其他变量，都是值传递。 Go 是支持面向对象的，但却没有继承的概念，在结构体中，可以通过组合其他结构体来构建更复杂的结构体。 结构体不能包含自己，但可以包含自己的指针 方法 在 Go 语言中，将函数绑定到具体的类型中，则称该函数是该类型的方法。 func NewMember(name string) *Member { return &Member{ \"name\": name, } } func (m *Member) setName(name string) { m.Name = name } 调用结构体的方法，与调用字段一样m := Member{} m.setName(\"小明\") 组合 Go 语言的结构体没有继承的概念，但是有组合的概念，通过这种方式来达到代码复用效果。 组合可以理解为定义一个结构体中，其字段可以是其他的结构体，这样，不同的结构体就可以共用相同的字段。 例如，我们定义了一个名为Animal表示动物，如果我们想定义一个结构体表示猫，如： type Animal struct { Name string } func (a Animal) Run() { fmt.Println(a.Name + \"is running\") } func (a Animal) Eat() { fmt.Println(a.Name + \"is eating\") } type Cat struct { a Animal } func main() { c := Cat { a: Animal { Name: \"猫\", }, } fmt.Println(c.a.Name) c.a.Run() } 匿名字段 上例中，Animal 作为 Cat 的字段时的变量名为 a，所以调用方法是 c.a.Run()。 Go 语言支持直接将类型作为结构体的字段，而不需要取变量名，这种字段叫匿名字段。 type Lion struct { Animal // 匿名字段 } func main(){ var lion = Lion{ Animal{ Name: \"狮子\", }, } lion.Run() fmt.Println(lion.Name) } powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"golang/2.语言特点.html":{"url":"golang/2.语言特点.html","title":"语言特点","keywords":"","body":"语言特点 简介 Go 是 Google 在 2009 年推出的一种静态强类型、编译型、并发型，并具有垃圾回收功能的开源编程语言，可轻松构建简单、可靠且高效的软件。 不过，为了方便搜索和识别，有时会将其称为 Golang。 Go 语言有时候被描述为 21 世纪的 C 语言，Go 从 C 语言继承了相似的表达式语法、控制流结构、基础数据类型、调用参数传值、指针等很多思想。 还有 C 语言一直所看中的编译后机器码的运行效率以及和现有操作系统的无缝适配。 但是在 Go 语言的家族树中还有其它的祖先。 其中一个有影响力的分支来自 Pascal 语言。 然后 Modula-2 语言激发了包的概念。 然后Oberon语言摒弃了模块接口文件和模块实现文件之间的区别。 第二代的 Oberon-2 语言直接影响了包的导入和声明的语法，还有 Oberon 语言的面向对象特性所提供的方法的声明语法等。 Go 语言的另一支祖先，带来了 Go 语言区别其他语言的重要特性，灵感来自于 CSP。 在 CSP 中，程序是一组中间没有共享状态的平行运行的处理过程，它们之间使用管道进行通信和控制同步。 Go 语言的其他的一些特性零散地来自于其他一些编程语言。 比如 iota 语法是从 APL 语言借鉴，词法作用域与嵌套函数来自于 Scheme 语言（和其他很多语言）。 当然 Go 也有很多创新的设计，比如切片为动态数组提供了有效的随机存取的性能，这可能会让人联想到链表的底层的共享机制。 还有 Go 语言新发明的 defer 语句。 自动垃圾回收 从 C 到 C++，从程序性能的角度来考虑，这两种语言允许程序员自己管理内存，包括内存的申请和释放等。 因为没有垃圾回收机制，所以 C/C++ 运行起来速度很快，但是随着而来的是程序员对内存使用上的很谨小慎微的考虑。 因为哪怕一点不小心就可能会导致内存泄露使得资源浪费或者野指针使得程序崩溃等。 为了提高程序开发的速度以及程序的健壮性，Java 和 C# 等高级语言引入了 GC 机制，由语言特性提供垃圾回收器来回收内存。 但是随之而来的可能是程序运行效率的降低。 Go 语言作为一门新生语言，当然不能忽略内存管理这个问题。 更丰富的内置类型 其实作为一种新兴的语言，如果仅仅是为了某种特定的用途那么可能其内置类型不是很多，仅需要能够完成基本功能即可。 但是 Go 语言不仅支持几乎所有语言都支持的简单内置类型（比如整型和浮点型等）外，还支持一些其他的高级类型，比如字典、切片。 函数多返回值 在 C/C++ 中，包括其他的一些高级语言是不支持多个函数返回值的。 但是这项功能又确实是需要的，所以它们一般通过将返回值定义成一个结构体，或者通过函数的参数引用的形式进行返回。 Go 语言不需要那么麻烦，它支持函数多返回值。 错误处理 在传统的 OOP 编程中，为了捕获程序的健壮性需要捕获异常，使用的方法大都是 try-catch。 Go 中的错误处理不需要这样，且 Go 语言中只有 error panic，没有 warn。 但是在当前版本中，Go 处理错误是这样的。 if err != nil{ // handle err } 这种错误处理会非常多，被很多人所排斥。不过我觉得还行，起码必须要去处理错误，只是没有一种优雅的方法，且多数人都不愿意去处理错误。 对于 Go 的错误处理一直有人在 建议， 但 提出的建议 并不怎么样。 过分的简单往往是一种负担。 匿名函数和闭包 在 Go 语言中，所有的函数也是值类型，可以作为参数传递。 f := func(x, y int) int { return x + y } 类型和接口 type Bird struct { // ... } func (b *Bird) Fly() { // ... } type IFly interface { Fly() } func main() { var fly IFly = new(Bird) fly.Fly() } 并发编程 Go 语言引入了 goroutine 概念，它使得并发编程变得非常简单。 通过使用 goroutine 而不是裸用操作系统的并发机制，以及使用消息传递来共享内存而不是使用共享内存来通信，并发编程变得更加轻盈和安全。 通过在函数调用前使用关键字 go，即可开启一个协程。 同时，Go 语言实现了 CSP 来作为 goroutine 间的推荐通信方式。 在 CSP 模型中，一个并发系统由若干并行运行的顺序进程组成，每个进程不能对其他进程的变量赋值。 进程之间只能通过一对通信原语实现协作。Go 语言用 channel 这个概念来轻巧地实现了CSP模型。 channel 的使用方式比较接近 Unix 系统中的 pipe 概念，可以方便地进行跨 goroutine 的通信。 反射 这里的反射和 java 中的反射类似，可以用来获取对象类型的相信信息，并动态操作对象。 因为反射可能会对程序的可读性有很大的干扰，所以，在 Go 中只是在特别需要反射支持的地方才实现反射的一些功能。 反射最常见的使用场景是做对象的序列化，例如 encoding/json encoding/xml encoding/gob encoding/binary 等包就大量依赖于反射功能来实现。 而且反射性能不佳，能有其他更好的实现方式就不要用。 语言交互性 这里的交互性主要是和 C 的交互性，之所以这样是因为 Go 语言的开发者是最初贝尔实验室创建Unix系统以及C语言的一般人，包括： Go 语言中重用了大部份的 C 模块，这里称为 Cgo.Cgo 允许开发者混合编写 C 语言代码 Cgo 工具可以将这些混合的 C 代码提取并生成对于 C 功能的调用包装代码。 开发者基本上可以完全忽略这个Go语言和C语言的边界是如何跨越的。 例如书中一个例子，在Go语言中直接调用了C标准库的puts函数。 package main /* #include */ import \"C\" import \"unsafe\" func main() { cstr := C.CString(\"Hello, world\") C.puts(cstr) C.free(unsafe.Pointer(cstr)) } powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/1.数据库.html":{"url":"数据库/1.数据库.html","title":"数据库","keywords":"","body":"数据库 数据库，又称为数据管理系统，用户可以对文件中的数据运行新增、截取、更新、删除等操作。 所谓“数据库”系以一定方式储存在一起、能予多个用户共享、具有尽可能小的冗余度、与应用程序彼此独立的数据集合。 一个数据库由多个表空间（Tablespace）构成。 数据库的分类 关系数据库 非关系型数据库 键值数据库 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/1.1.范式.html":{"url":"数据库/1.1.范式.html","title":"范式","keywords":"","body":"范式 在关系数据库中，设计数据库时遵从一定的规范要求，可以使得数据库冗余变小。 目前关系数据库有六种范式，不过查了查资料，其实是十种。 1NF 第一范式 2NF 第二范式 3NF 第三范式 EKNF 主键范式 BCNF Boyce–Codd 范式 4NF 第四范式 ETNF 关键元组范式 5NF 第五范式 DKNF 域键范式 6NF 第六范式 不过在这里呢，不讲 EKNF 和 ETNF，以八大范式的形式来讲。 1 2 3 BC 4 5 DK 6 主键 ✔️ ✔️ ✔️ ✔️ ✔️ ✔️ ✔️ ✔️ 没有重复组 ✔️ ✔️ ✔️ ✔️ ✔️ ✔️ ✔️ ✔️ 字段原子性 ✔️ ✔️ ✔️ ✔️ ✔️ ✔️ ✔️ ✔️ 没有部分函数依赖 ❌ ✔️ ✔️ ✔️ ✔️ ✔️ ✔️ ✔️ 没有传递函数依赖 ❌ ❌ ✔️ ✔️ ✔️ ✔️ ✔️ ✔️ 去除主属性对不必要的函数依赖 ❌ ❌ ❌ ✔️ ✔️ ✔️ ✔️ - 每个非平凡的多值依赖都有一个超键 ❌ ❌ ❌ ❌ ✔️ ✔️ ✔️ - 超键是每个显式连接依赖的一部分 ❌ ❌ ❌ ❌ ❌ ✔️ ✔️ - 候选键隐含了每个非平凡的连接依赖关系 ❌ ❌ ❌ ❌ ❌ ✔️ ✔️ - 每个约束都是域约束和键约束的结果 ❌ ❌ ❌ ❌ ❌ ❌ ✔️ - 每个连接依赖都是平凡的 ❌ ❌ ❌ ❌ ❌ ❌ ❌ ✔️ ✔️: 符合 ❌: 不符合 -: 不适用 1NF 在上表中可以看到，第一范式要求数据库的每个列的值域都是由原子值组成；每个字段的值都只能是单一值，当然还要有主键。 重复组 顾客 日期 消费 张三 1970-01-01 80.00 90.00 10.00 李四 1970-01-01 16.00 王五 1970-01-01 3.00 这种情况下就是不符合第一范式的，消费就是重复组。想要消除重复组的话，只要把每笔记录都转化为单一记录即可： 顾客 日期 消费 张三 1970-01-01 80.00 张三 1970-01-01 90.00 张三 1970-01-01 10.00 李四 1970-01-01 16.00 王五 1970-01-01 3.00 主键 上表是没有主键的，如果同一天同一个人消费呢同样的金额，这样的交易做了两次，这个表就有点乱了。 因为这两笔交易是一模一样的，也就是说如果只靠这些数据没有办法分辨这两笔记录。 之所以说它不符合第一范式，是因为上面这样的表示法欠缺一个唯一标识符，而且可以保证在这个数据中唯一标识符不会重复出现。 交易 顾客 日期 消费 1 张三 1970-01-01 80.00 2 张三 1970-01-01 90.00 3 张三 1970-01-01 10.00 4 李四 1970-01-01 16.00 5 王五 1970-01-01 3.00 字段原子性 有时候为了方便，可能设计数据库的时候会这样。 姓名 联系方式 张三 12345678901, 12345@mail.com 张三 12345678902 但是这样就违反了字段的原子性，改成这样会更好。 姓名 手机号 邮箱 张三 12345678901 12345@mail.com 张三 12345678902 再加上主键吧。 编号 姓名 手机号 邮箱 0 张三 12345678901 12345@mail.com 1 张三 12345678902 2 李四 12345678903 12346@mail.com 2NF 第二范式在第一范式的基础上增加了一点要求，数据表里的所有数据都要和该数据表的键（主键与候选键）有完全依赖关系： 每个非键属性必须独立于任意一个候选键的任意一部分属性。 如果有哪些数据只和一个键的一部分有关的话，就得把它们独立出来变成另一个数据表。 商品 ID(Primary key) 价格 供应商 ID(Primary key) 供应商名称 1 10.00 1 食品公司 2 20.00 1 食品公司 3 33.33 2 饮料公司 这个数据表的每个值都是单一值，所以它符合第一范式。 因为同一个商品有可能由不同的供应商提供，所以得把商品 ID 和供应商 ID 合在一起组成一个主键。 商品和价格之间的关系很正确：同一个商品在不同供应商有可能会有不同的报价，所以价格确实和主键完全相关。 另一方面，供应商的名称只和供应商 ID 有关，这不符合第二范式的原则。 商品 ID 价格 供应商 ID 1 10.00 1 2 20.00 1 3 33.33 2 供应商 ID 供应商名称 1 食品公司 2 饮料公司 3NF 第三范式要求所有非主键属性都只和候选键有相关性，也就是说非主键属性之间应该是独立无关的。 学号 姓名 年龄 所在学院 学院地点 学院电话 1 张三 1 语言文化 一路向西 12345678 2 张三 1 语言文化 一路向西 12345678 3 李四 2 科学技术 一路向南 12345679 这个数据表就是符合第二范式但不符合第三范式的，因为其中存在 (学号) → (所在学院) → (学院地点, 学院电话) 的关系。 它存在非关键字段\"学院地点\"、\"学院电话\"对关键字段\"学号\"的传递函数依赖。 学号 姓名 年龄 学院编号 1 张三 1 1 2 张三 1 1 3 李四 2 2 学院编号 学院名称 学院地点 学院电话 1 语言文化 一路向西 12345678 2 科学技术 一路向南 12345679 BCNF 如果对第三范式做进一步加强，那就是 Boyce-Codd 范式了。 BC 范式去除了属性间的不必要的函数依赖。 商店 ID 商品 ID 数量 店长 1 1 88 张三 1 2 99 张三 2 3 7107 李四 2 4 5511 李四 2 5 4114 李四 对于这张表，存在如下关系： 一个商店对应一个店长 一个商店有多个商品 想让其符合 BCNF，可以改成这样。 商店 ID 商品 ID 数量 1 1 88 1 2 99 2 3 7107 2 4 5511 2 5 4114 商店 ID 店长 1 张三 2 李四 4NF 前几个方式都是关注属性集合之间的函数依赖，而第四范式关注更一般形式。 第四范式能作用到的范围有限，不是什么场景都能用得到的。 而且比较扯。 学生 课程 爱好 张三 语文 篮球 张三 语文 网球 张三 数学 网球 李四 语文 网球 王五 语文 网球 学生 课程 张三 语文 张三 数学 李四 语文 王五 语文 学生 爱好 张三 篮球 张三 网球 李四 网球 王五 网球 5NF 第五范式去除多个关系之间的语义相关。 客户 品牌 产品 张三 ABC S 张三 ABC G 张三 KCF W 李四 KCF W 李四 ABC G 如果数据表在 4NF 中并且不包含任何连接依赖关系并且连接应该是无损的，则关系在 5NF 中。当所有表都被分成尽可能多的表以便避免冗余时，满足 5NF。 客户 品牌 张三 ABC 张三 KCF 李四 KCF 李四 ABC 客户 产品 张三 S 张三 G 张三 W 李四 W 李四 G 品牌 产品 ABC S ABC W ABC G KCF W ABC G DKNF DKNF 是去除关系不包含于域约束的其他约束。 如果经验与等级的换算是 等级 * 100，类型有 NPC 和 Player。这张表就是违反 DKNF 的。 玩家 类型等级 经验值 张三 MPC.3 322 李四 Player.8 858 王五 Player.4 425 可以分离为两张表。 玩家 类型 经验值 张三 MPC 322 李四 Player 858 王五 Player 425 等级 经验 3 300 8 800 4 400 6NF 第六范式，所谓每个连接依赖都是平凡的，通俗来说就是每个表都只能拥有一个主键和不超过一个的属性。 这就意味着，每张表的列数非常的少，除了主键之外，最多只能拥有一列。 看到这个定义之后，是不是有点 No-SQL key-value 数据库的意思。 数据是随着时间不断变化的，也就是我们常说的一句谚语：唯一不变的只有变化本身。 这就要求数据库设计最好是模块化，比较灵活，而且很容易能够跟踪数据的变化。 很可惜，现有的五范式并不能满足这些要求。 这里并不讲第六方式，不是因为别的，因为没这么接触过，不甚了解。 真正的范式 如果你知道自己在做什么，就去做吧，不要让规则束缚了你自己 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/1.2.ACID.html":{"url":"数据库/1.2.ACID.html","title":"ACID","keywords":"","body":"ACID ACID 是指数据库管理系统（DBMS）在写入或更新资料的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性。 原子性 Atomicity，原子性，又称不可分割性。 一个事务中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。 事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。 即，事务不可分割、不可约简。 一致性 Consistency，一致性。 在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。 隔离性 Isolation，隔离性，又称独立性。 数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。 事务隔离分为不同级别，包括未提交读、提交读、可重复读和串行化。 持久性 Durability，持久性。 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 如果没有 ACID 为了更好地理解 ACID，可以以银行账户转账举例： A 账户有 1000，B 账户有 300，A 给 B 转账 500。 原子性。转账过程中，因为 B 的账户不存在，交易失败了。那么 A 的余额减少了，B 的余额却没增加。 一致性。转账过程中，A 的余额刚刚被扣除，数据库系统崩溃了，金额不翼而飞。 隔离性。转账过程中，转账还未完成，A 查询自己账户就已经显示被扣款了。 持久性。这个很好理解，数据库所在服务器断电了。 题外话，事务的隔离性是通过锁、MVCC 等实现的，事务的原子性、一致性和持久性则是通过事务日志实现。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/1.3.扩展.html":{"url":"数据库/1.3.扩展.html","title":"扩展","keywords":"","body":"扩展 关于数据库的扩展，一般分为两类，水平扩展和垂直扩展。 对于许多类型的应用，传统的解决方法是购买更多强悍的机器，也就是常说的垂直扩展或者向上扩展。 另外一个与之相反的方法是将任务分配到多台计算机上，这通常称为水平扩展或者向外扩展。 垂直扩展 对于垂直扩展，意味着购买更多性能强悍的硬件，对很多应用来说这是唯一需要做的事情。 但是在现代硬件上 MySQL 能扩展的合理值为 256 GB RAM，32 核 CPU 以及一个 PCIE flash 驱动器，并且需要使用尽可能最新的 MySQL 版本。 如果在此基础上继续提升硬件的配置，MySQL 的性能虽然还能提升，但性价比就会降低。 水平扩展 一般策略划分为三个部分：复制、拆分、以及数据分片。 最简单也最常见的水平扩展的方法是通过复制将数据分发到多个服务器上，然后将备库用于读查询。 这种技术对于以读为主的应用很有效。但也有一些缺点，例如重复缓存，不过如果数据规模有限就不存在这个问题。 在 MySQL 架构中，一个节点就是一个功能组件。 如果没有规划冗余和高可用性，那么一个节点可能就是一台服务器。 如果设计的是能够故障转移的冗余系统，那么一个节点通常可能就是下面的某一种： 主从复制双机结构 一主多备 一主，并使用分布式复制块设备（DRBD）作为备用服务器 一个基于存储区域网络（SAN）的集群 在大多数情况下，一个节点内的所有服务器应该拥有相同的数据。 按功能划分 数据库拆分一般可以按照功能拆分或者说是业务拆分，也就是把不同的功能或者是不同的业务尽可能地拆分开， 然后把各个功能或业务需要的数据库独立运行，也是一种很好的方式。可想而已肯定可以减小写入和读取数据的压力。 数据分片 在目前用于扩展大型 MySQL 应用的方案中，数据分片是最通用且最成功的方法。 把数据分割成一小片，或者说一块，然后存储到不同的节点中。 数据分片在和某些类型的按功能划分联合使用时非常有用。 大多数分片系统也有一些全局的数据不会被分片（例如城市列表或者登陆数据）。 全局数据一般存储在单个节点上，并且通常保存在类似 memcached 这样的缓存里。 分片数据存储看起来像是优雅的解决方案，但除非是在项目架构阶段就考虑过并留有扩展空间，不然很难实现。 为什么要选择这个架构呢？因为如果想扩展写容量，就必须切分数据。如果只有单台主库，那么不管有多少备库，写容量都是无法扩展的。 选择分区键 选择分区键的时候，尽可能选择那些能够避免跨分片查询的，但同时也要让分片足够小，以避免过大的数据片导致问题。 如果可能，应该期望分片尽可能同样小，这样在为不同数量的分片进行分组时能够很容易平衡。 多个分区键 跨分片查询 分配数据、分片和节点 在节点上部署分片 固定分配 将数据分配到分片中有两种主要的方法：固定分配和动态分配。 两种方法都需要一个分区函数，使用行的分区健值作为输入，返回存储该行的分片。 固定分配使用的分区函数仅仅依赖于分区键的值，哈希函数和取模运算就是很好的例子。 但固定分配也有缺点，如果分片很大并且数量不多，就很难平衡不同分片间的负载。 修改分片策略比较困难，因为需要重新分配已有的数据。 动态分配 另外一个选择使用动态分配，将每个数据单元映射到一个分片。假设一个有两列的表，包括用户ID和分片ID。 create table user_to_shard { user_id INT NOT NUll, shard_id INT NOT NULL primary key (user_id) } 这个表本身就是分区函数。给定分区键（用户ID）的值就可以获取分片号。 动态分配增加了分区函数的开销，因为需要额外调用一次外部资源。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/1.4.冗余.html":{"url":"数据库/1.4.冗余.html","title":"冗余","keywords":"","body":"冗余 什么是数据库中的数据冗余？ 在一个数据集合中重复的数据称为数据冗余。 例如在设计数据库时，某一字段属于一个表，但它又同时出现在另一个或多个表，且完全等同于它在其本来所属表的意义表示，那么这个字段就是一个冗余字段。 关系数据库中的数据冗余主要是指关系数据库中同一信息数据的重复存贮。 数据冗余浪费了宝贵的资源，应尽量减少。但关系数据库中为实现一些功能有些数据冗余是必需的。 必需的数据冗余主要用于以下用途： 数据间建立联系，如两表间通过共同属性建立联系 数据恢复，如建立备份文件以备正式文件被破坏时恢复 数据核查，如设立数据校验位可以检查数据在存贮、传输等过程中的改变 数据使用的便利，如为了查看数据的直观，使用数据的方便、高效 减少数据通讯开销，如分布式数据库在不同场地重复 数据冗余的原因 关系数据库由表及附属文件组成，其表由属性定义的结构和记录组成，其属性值域有多种类型。 关系数据库的数据冗余形成的原因有表的重复、属性的重复、元组的重复、属性值的重复。 表的重复 为了数据安全的需要制作备份表，当主表被破坏时可用此恢复数据。 分布式数据库为减少数据通讯开销也常重复放表，这种数据冗余在这里是必需数据冗余，不能删除。 若是因其他原因产生的非必要的重复表则应予以删除。 属性重复 有不同表的属性重复和同一表内属性重复两种情况： 不同表中属性重复常用来建立表之间联系，这只需要一个公共属性，这是必需数据冗余，不能删除；各表间的多于一个的属性应当删除。 T1(A,B,C)；T2(A,B,D)；T3(A,C,D,E) 其中 A 为三表所共有；B 为 T1 T2 共有，C 为 T1 T3 共有；D 为 T2 T3 共有。 如取 A 为公共属性则，T1 T2 两张表中只能保留一个 B 属性，T1 T3 两张表中只能保留一个 C 属性，T2 T3 两张表中只能保留一个 D 属性。 同一表内有相同属性内容的多个属性，若非数据安全检查的需要，应该删除。 元组的重复 表内不同记录内容有时会完全相同，若非必要，应予以删除。 属性值的重复 按属性值域集合基的特点可以将其分为有限类和无限类。 无限类属性值是指其属性值域集合的基为无限大或者数据库记录数为同一数量级的属性值，如实数、整数、日期、各种编号。 无限类属性值偶尔也可能重复，但这只是巧合，而并非数据冗余。 有限类属性值是指其属性值域集合的基小于数据库记录数至少一个数量级的属性值，如产品名，部门名，职称名，课程名。 有限类属性值的重复实际上是由一对多或多对多的关系引起的，有时可作为必需冗余数据不予以处理， 这时不需程序就有较好的查看效果和工作效率。但当重复量很大时，也应当设法对所引起的数据冗余进行压缩，这通常要建立新表和相应的程序。 数据冗余的缺点 存储空间的浪费 数据交互和数据库访问执行效率降低 但适当的数据冗余又能加快查询，数据冗余究竟是好是坏还是要根据自己所做的项目进行合理的取舍。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/1.5.ID.html":{"url":"数据库/1.5.ID.html","title":"分布式 ID","keywords":"","body":"分布式 ID 分布式数据库，分成多个表之后，需要一个全局唯一的 ID。 UUID 不适合作为主键，因为太长了，并且无序不可读，查询效率低。比较适合用于生成唯一的名字的标示比如文件的名字。 自增 ID 两台数据库分别设置不同步长，生成不重复 ID 的策略来实现高可用。 这种方式生成的 ID 有序，但是需要独立部署数据库实例，成本高，还会有性能瓶颈。 利用 Redis 生成 ID 性能好，灵活方便，不依赖于数据库。 但引入了新的组件造成系统更加复杂，可用性降低，编码更加复杂，增加了系统成本。 Snowflake Twitter 的 Snowflake算法 https://github.com/twitter-archive/snowflake Leaf 美团的Leaf分布式ID生成系统， 能保证全局唯一性、趋势递增、单调递增、信息安全，里面也提到了几种分布式方案的对比，但也需要依赖关系数据库、Zookeeper等中间件。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/2.简介.html":{"url":"数据库/2.简介.html","title":"MariaDB","keywords":"","body":"简介 MariaDB 是 MySQL 的分支版本，由社区开发，有商业支持，在 GNU GPL 下开源。 它主要是由于 MySQL 在被 Oracle 公司收购时出现的问题而开发的。 MariaDB 是一个通用的数据库管理系统（DBMS），它具有可扩展的架构，可通过可插拔存储引擎支持大量的用例。 它使用不同的存储引擎来支持不同的用例。 MariaDB 打算保持与 MySQL 的高度兼容性，确保具有库二进制奇偶校验的直接替换功能，以及与 MySQL API 和命令的精确匹配。 MariaDB 自带了一个新的存储引擎 Aria，它可以替代 MyISAM 成为默认的事务和非事务引擎。 MariaDB 是世界上最流行的开源数据库之一。 许多 Linux 发行版都是用它作为默认数据库， 例如：Debian Ubuntu Arch Manjaro openSUSE RedHat CentOS Fedora SUSE powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/2.1.引擎.html":{"url":"数据库/2.1.引擎.html","title":"引擎","keywords":"","body":"引擎 先查看一下数据库有什么引擎吧 MariaDB [(none)]> show engines; +--------------------+---------+----------------------------------------------------------------------------------+--------------+------+------------+ | Engine | Support | Comment | Transactions | XA | Savepoints | +--------------------+---------+----------------------------------------------------------------------------------+--------------+------+------------+ | MRG_MyISAM | YES | Collection of identical MyISAM tables | NO | NO | NO | | CSV | YES | Stores tables as CSV files | NO | NO | NO | | MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO | | MyISAM | YES | Non-transactional engine with good performance and small data footprint | NO | NO | NO | | Aria | YES | Crash-safe tables with MyISAM heritage | NO | NO | NO | | InnoDB | DEFAULT | Supports transactions, row-level locking, foreign keys and encryption for tables | YES | YES | YES | | PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO | | SEQUENCE | YES | Generated tables filled with sequential values | YES | NO | YES | +--------------------+---------+----------------------------------------------------------------------------------+--------------+------+------------+ InnoDB 自从某个版本之后，InnoDB 就是 MariaDB 以及 MySQL 的默认存储引擎了，在这之前是 MyISAM。 InnoDB 使用行级锁，提供了具有提交、回滚和崩溃回复能力的事务安全。 支持自动增长列，支持外键约束，并发能力强，占用空间是 MYISAM 的 2.5 倍，处理效率相对会差一些。 一般来说，InnoDB 对于多数情况都是很不错的选择，能满足绝大多数的需要。 MyISAM 全表锁，拥有较高的执行速度，不支持事务，不支持外键，并发性能差，支持压缩，占用空间相对较小。 对事务完整性没有要求，以 select、insert 为主的应用基本上可以使用这引擎。 MRG_MyISAM MRG_MyISAM 引擎是一组 MyISAM 表的组合，在 MySQL5.7 之前叫做 MEGER。 MEG_MyISAM 引擎需要主表 MERGE 和子表具有完全一样的数据结构（属性、数据类型、索引）。 对于存储策略是分成多表，如日志表，按照一年十二个月份划分，需要 12 张定义完全一样的数据表。 当联合多个月份的表查询数据时，需要写多个表的连接，这时使用 MEG_MyISAM 引擎只需要像对待一张表一样查询，对数据的操作就变得简单了。 MEMORY MEMORY 使用全表锁，速度快，数据直接存储在内存中，但会占用和数据量成正比的内存空间。且数据在mysql重启时会丢失。 默认使用 HASH 索引，检索效率非常高，但不适用于精确查找，主要用于那些内容变化不频繁的代码表。 InnoDB 与 MyISAM 的区别 事务 InnoDB 支持事务，MyISAM 不支持。 InnoDB 默认把每一条 SQL 都封装成事务，这样会影响速度，所以最好把多条 SQL 组成一个事务。 外键 InnoDB 支持外键，MyISAM 不支持。 对一个包含外键的 InnoDB 表转为 MYISAM 会失败。 索引 InnoDB 是聚集索引，数据文件是和索引绑在一起的，必须要有主键，通过主键索引效率很高。 但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。 因此，主键不应该过大，因为主键太大，其他索引也都会很大。 MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 全表查询 InnoDB 不保存表的具体行数，执行 select count(*) from table 时需要全表扫描。 MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快. 细节差异 InnoDB 不支持 FULLTEXT 类型的索引 对于 AUTO_INCREMENT 类型的字段，InnoDB 中必须包含只有该字段的索引，但是在 MyISAM 表中，可以和其他字段一起建立联合索引 DELETE FROM table，InnoDB 不会重新建立表，而是一行一行的删除 InnoDB 的行锁不是绝对的，一个 SQL 语句不能确定要扫描的范围时会锁全表，例 update table set num=1 where name like “%aaa%” 怎样选择数据库引擎 特性 InnoDB MyISAM MEMORY 储存限制 64TB 无 RAM 锁机制 行 表 表 外键 ✔️ ❌ ❌ 事务 ✔️ ❌ ❌ B+ 树索引 ✔️ ✔️ ✔️ 全文索引 ✔️ ✔️ ❌ 哈希索引 ✔️ ❌ ✔️ 集群索引 ✔️ ❌ ❌ 数据缓存 ✔️ ❌ - 索引缓存 ✔️ ✔️ - 数据压缩 ✔️ ✔️ - 储存消耗 高 低 - 内存消耗 高 低 - 批量写入 慢 快 快 一般情况 InnoDB 和 MyISAM 都可以使用，深入一点的就看是否需要事务和外键来决定。 MyISAM 没办法抗并发写操作，不过可以通过架构来弥补的嘛。 都是看应用场景了，项目不大、没有什么大流量，没什么架构的，InnoDB 就够了。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/2.2.事务.html":{"url":"数据库/2.2.事务.html","title":"事务","keywords":"","body":"事务 数据库事务通常包含了一个序列的对数据库的读写操作。包含有以下两个目的： 为数据库操作序列提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。 当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。 当事务被提交给了数据库管理系统，需要确保该事务中的所有操作都成功完成且其结果被永久保存在数据库中。 如果事务中有的操作没有成功完成，则事务中的所有操作都需要回滚，回到事务执行前的状态。 同时，该事务对数据库或者其他事务的执行无影响，所有的事务都好像在独立的运行。 特点 从业务角度出发，对数据库的一组操作要求保持 4 个特征：原子性、一致性、隔离性、持久性。 为了更好地理解 ACID，可以看 ACID 专文。 隔离级别 并发事务带来的问题 更新丢失： 当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题。 最后的更新覆盖了由其他事务所做的更新，例如，两个编辑人员制作了同一文档的电子副本。 每个编辑人员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。 最后保存其更改副本的编辑人员覆盖另一个编辑人员所做的更改。 如果在一个编辑人员完成并提交事务之前，另一个编辑人员不能访问同一文件，则可避免此问题。 脏读： 一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致状态。 另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些脏数据，并据此做进一步的处理，就会产生未提交的数据依赖关系。 不可重复读： 一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或被删除了。 幻读： 一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据。 幻读和不可重复读的区别： 不可重复读的重点是修改，在同一事务中，同样的条件，第一次读的数据和第二次读的数据不一样，因为中间有其他事务提交了修改。 幻读的重点在于新增或者删除，在同一事务中，同样的条件，第一次和第二次读出来的记录数不一样，因为中间有其他事务提交了插入、删除。 解决办法 更新丢失通常是应该完全避免的。 但防止更新丢失，并不能单靠数据库事务控制器来解决，需要应用程序对要更新的数据加必要的锁来解决，防止更新丢失应该是应用的责任。 脏读、不可重复读和幻读都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。 要么加锁，在读取数据前对其加锁，阻止其他事务对数据进行修改。 要么使用 MVCC，也称为多版本数据库，不用加任何锁。 它会通过一定机制生成一个数据请求时间点的一致性数据快照，并用这个快照来提供一定级别的一致性读取。 从用户的角度来看，好象是数据库可以提供同一数据的多个版本。 SQL 标准定义了四类隔离级别，每一种级别都规定了一个事务中所做的修改，哪些在事务内和事务间是可见的，哪些是不可见的。 低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。 第一级别：未提交读 所有事务都可以看到其他未提交事务的执行结果 本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少 该级别引发的问题是脏读，读取到了未提交的数据 第二级别：已提交读 这是大多数数据库系统的默认隔离级别，但不是 MariaDB、MySQL 默认的 它满足了隔离的简单定义，一个事务只能看见已经提交事务所做的改变 这种隔离级别出现的问题是不可重复读，意味着在同一个事务中执行相同的 select 语句时可能看到不一样的结果。 导致这种情况的原因可能有： 有一个交叉的事务有新的 commit，导致了数据的改变; 一个数据库被多个实例操作时，同一事务的其他实例在该实例处理其间可能会有新的 commit 第三级别：可重复读 这是 MariaDB、MySQL 默认的事务隔离级别 它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行 此级别可能出现的幻读，当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当再读取该范围的数据行时，会出现幻读 InnoDB 通过 MVCC 机制、间隙锁解决幻读问题 MVCC 大多数据库事务型引擎实现都不是简单的行级锁。 基于提升并发性考虑，一般都同时实现了 MVCC，包括 Oracle、PostgreSQL，不过实现各不相同。 MVCC 的实现是通过保存数据在某一个时间点快照来实现的。也就是不管实现时间多长，每个事物看到的数据都是一致的。 分为乐观并发控制和悲观并发控制。 InnoDB 的 MVCC 通过在每行记录后面保存两个隐藏的列来实现。 这两个列一个保存了行的创建时间，一个保存行的过期时间。 当然存储的并不是真实的时间而是系统版本号。 每开始一个新的事务，系统版本号都会自动新增。 事务开始时刻的系统版本号会作为事务的版本号，用来查询到每行记录的版本号进行比较。 第三隔离级别下 MVCC 如何工作： SELECT 只查找版本早于当前事务版本的数据行，这样可以确保事务读取的行要么是在开始事务之前已经存在要么是事务自身插入或者修改过的。 行的删除版本号要么未定义，要么大于当前事务版本号，这样可以确保事务读取到的行在事务开始之前未被删除。 只有符合上述两个条件的才会被查询出来。 INSERT 为新插入的每一行保存当前系统版本号作为行版本号。 DELETE 为删除的每一行保存当前系统版本号作为行删除标识。 UPDATE 为插入的一行新纪录保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为删除标识。 保存这两个版本号，使大多数操作都不用加锁。使数据操作简单，性能很好，并且能保证只会读取到复合要求的行。 不足之处是每行记录都需要额外的存储空间，需要做更多的行检查工作和一些额外的维护工作。 MVCC 只在第二三两种隔离级别下工作。 可以认为MVCC是行级锁一个变种，但是他很多情况下避免了加锁操作，开销更低。 虽然不同数据库的实现机制有所不同，但大都实现了非阻塞的读操作，读不用加锁，且能避免出现不可重复读和幻读。 写操作也只锁定必要的行，写必须加锁，否则不同事务并发写会导致数据不一致。 第四级别：可串行读 这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题 它在每个读的数据行上加上共享锁 在这个级别，可能导致大量的超时现象和锁竞争 隔离级别 隔离级别 脏读 丢失更新 不可重复读 幻读 未提交读 ✔️ ✔️ ✔️ ✔️ 已提交读 ❌ ❌ ✔️ ✔️ 可重复读 ❌ ❌ ❌ ✔️ 可串行读 ❌ ❌ ❌ ❌ 各数据库并不一定完全实现了四个隔离级别。 Oracle 只提供 Read committed 和 Serializable 两个标准隔离级别，另外还提供自己定义的 Read only 隔离级别。 SQL Server 除支持这些级别外，还支持一个叫做快照的隔离级别，但严格来说它是一个用 MVCC 实现的 Serializable 隔离级别。 MariaDB、MySQL 都支持，但在具体实现时，有一些特点，比如在一些隔离级别下是采用 MVCC 一致性读，但某些情况下又不是。 Mysql可以通过执行 set transaction isolation level 命令来设置隔离级别，新的隔离级别会在下一个事务开始的时候生效。 例如：set session transaction isolation level read committed; 事务日志 使用事务日志，存储引擎在修改表的数据时只需要修改其内存拷贝，再把该修改行为记录到持久在硬盘上的事务日志中。 不用每次都将修改的数据本身持久到磁盘。 事务日志采用的是追加的方式，因此写日志的操作是磁盘上一小块区域内的顺序 I/O ，而不像随机I/O需要在磁盘的多个地方移动磁头。 采用事务日志的方式相对来说要快得多。 事务日志持久以后，内存中被修改的数据在后台可以慢慢刷回到磁盘。 如果数据的修改已经记录到事务日志并持久化，但数据本身没有写回到磁盘，此时系统崩溃，存储引擎在重启时能够自动恢复这一部分修改的数据。 目前来说，大多数存储引擎都是这样实现的，通常称之为预写式日志，修改数据需要写两次磁盘。 实现原理 事务的实现是基于数据库的存储引擎，不同的存储引擎对事务的支持程度不一样。 InnoDB 是 MariaDB 默认的存储引擎，默认的隔离级别是可重复读，并且在可重复读的隔离级别下更进一步，加入了 MVCC 和 间隙锁。 因此 InnoDB 的可重复读隔离级别其实实现了串行化级别的效果，而且保留了比较好的并发性能。 事务的隔离性是通过锁实现，而事务的原子性、一致性和持久性则是通过事务日志实现。说到事务日志就得提一下 redo 和 undo。 redo log 事务日志通过重做 (redo) 日志和日志缓冲 (InnoDB Log Buffer) 实现。 事务开启时，事务中的操作都会先写入存储引擎的日志缓冲中，在事务提交之前，这些缓冲的日志都需要提前刷新到磁盘上持久化。 这就是预写日志。 当事务提交之后，在 Buffer Pool 中映射的数据文件才会慢慢刷新到磁盘。 此时如果数据库崩溃或者宕机，那么当系统重启进行恢复时，就可以根据 redo log 中记录的日志，把数据库恢复到崩溃前的一个状态。 未完成的事务可以继续提交也可以选择回滚，这基于恢复的策略而定。 undo log undo log 主要为事务的回滚服务。 在事务执行的过程中，除了记录 redo log，还会记录一定量的 undo log。 undo log 记录了数据在每个操作前的状态，如果事务执行过程中需要回滚，就可以根据 undo log 进行回滚操作。 单个事务的回滚，只会回滚当前事务做的操作，并不会影响到其他的事务做的操作。 开始事务 记录 A=1 到 undo log update A = 3 记录 A=3 到 redo log 记录 B=2 到 undo log update B = 4 记录B = 4 到redo log 将 redo log 刷新到磁盘 提交 在 1 - 8 的任意一步系统宕机，事务未提交，该事务就不会对磁盘上的数据做任何影响。 在 8 - 9 系统宕机，恢复之后可以选择回滚，也可以选择继续完成事务提交，因为此时 redo log 已经持久化。 若在 9 之后系统宕机，内存映射中变更的数据还来不及刷回磁盘，那么系统恢复之后，可以根据 redo log 把数据刷回磁盘。 所以，redo log 其实保障的是事务的持久性和一致性，而 undo log 则保障了事务的原子性。 事务使用 START TRANSACTION 或 BEGIN 开始事务 COMMIT 提交 ROLLBACK 回滚 CHAIN 在事务提交或者回滚之后会立即启动一个新事务，并且和刚才的事务具有相同的隔离级别 RELEASE 在事务提交或者回滚之后会断开和客户端的连接。 SET AUTOCOMMIT 可以修改当前连接的提交方式，如果设置了 SET AUTOCOMMIT=0，则需要通过明确的命令进行提交或者回滚 注意点： 在锁表期间用 start transaction 命令开始一个新事务，会造成一个隐含的 unlock tables 被执行 在同一个事务中，最好不使用不同存储引擎的表，否则 ROLLBACK 时需要对非事务类型的表进行特别的处理，因为 COMMIT、ROLLBACK 只能对事务类型的表进行提交和回滚 所有的 DDL 语句是不能回滚的，并且部分的 DDL 语句会造成隐式的提交 在事务中可以通过定义 SAVEPOINT 指定回滚事务的一个部分，但是不能指定提交事务的一个部分。 对于复杂的应用，可以定义多个不同的 SAVEPOINT，满足不同的条件时，回滚不同的 SAVEPOINT。 需要注意的是，如果定义了相同名字的 SAVEPOINT，则后面定义的 SAVEPOINT 会覆盖之前的定义。 对于不再需要使用的 SAVEPOINT，可以通过 RELEASE SAVEPOINT 删除，删除后不能再执行 ROLLBACK TO SAVEPOINT。 默认采用自动提交模式，可以通过设置 autocommit 来启用或禁用自动提交模式 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/2.3.索引.html":{"url":"数据库/2.3.索引.html","title":"索引","keywords":"","body":"索引 索引是对数据库表中一个或多个列的值进行排序的结构，建立索引有助于快速获取信息。 你也可以这样理解，数据库的索引类似于书籍的索引。 在书籍中，索引允许用户不必翻阅完整个书就能迅速地找到所需要的信息。 在数据库中，索引也允许数据库程序迅速地找到表中的数据，而不必扫描整个数据库。 存储类型 B-Tree 索引 InnoDB 使用的是 B+ Tree。 B+ Tree 每一个叶子节点都包含指向下一个叶子节点的指针，从而方便叶子节点的范围遍历。 B+ Tree 通常意味着所有的值都是按顺序存储的，并且每一个叶子页到根的距离相同，很适合查找范围数据。 B-Tree 可以对 >= BETWEEN IN 以及不以通配符开始的 LIKE 使用索引。 Hash 索引 哈希索引基于哈希表实现，只有精确索引所有列的查询才有效。 对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码。 哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据的指针。 空间索引 MyISAM 支持空间索引，主要用于地理空间数据类型，例如 GEOMETRY。 全文索引 全文索引是 MyISAM 的一个特殊索引类型，它查找的是文本中的关键词，主要用于全文检索。 索引使用 建立索引类型 单列索引，即一个索引只包含单个列，一个表可以有多个单列索引，但这不是组合索引。 组合索引，即一个索包含多个列。 索引是在存储引擎中实现的，而不是在服务器层中实现的。 所以，每种存储引擎的索引都不一定完全相同，并不是所有的存储引擎都支持所有的索引类型。 普通索引 普通索引是最基本的索引，它没有任何限制。 普通索引（由关键字 KEY 或 INDEX 定义的索引）的唯一任务是加快对数据的访问速度。 因此，应该只为那些最经常出现在查询条件 (WHERE column = …) 或排序条件 (ORDER BY column) 中的数据列创建索引。 创建：CREATE INDEX indexName ON mytable(username(length)) 如果是 CHAR VARCHAR 类型，length 可以小于字段实际长度；如果是 BLOB 和 TEXT 类型，必须指定 length，下同。 修改表结构：ALTER mytable ADD INDEX [indexName] ON (username(length)) 创建表的时候直接指定：CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, INDEX [indexName] (username(length)) ) 删除：DROP INDEX [indexName] ON mytable 唯一索引 它与前面的普通索引类似，不同的是普通索引允许被索引的数据列包含重复的值，唯一索引列的值必须唯一，但允许有空值。 如果是组合索引，则列值的组合必须唯一。 创建：CREATE UNIQUE INDEX indexName ON mytable(username(length)) 修改表结构：ALTER mytable ADD UNIQUE [indexName] ON (username(length)) 创建表的时候直接指定：CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, UNIQUE [indexName] (username(length)) ); 主键索引 它是一种特殊的唯一索引，不允许有空值。一个表只能有一个主键。 一般是在建表的时候同时创建主键索引： CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, PRIMARY KEY(ID) ); 当然也可以用 ALTER 命令。 与之类似的，外键索引。 如果为某个外键字段定义了一个外键约束条件，MariaDB 就会定义一个内部索引来帮助自己以最有效率的方式去管理和使用外键约束条件。 组合索引 为了进一步提升效率，就要考虑建立组合索引。就是将多个字段建到一个索引里。 建立这样的组合索引，其实是相当于分别建立了下面三组组合索引： MariaDB 的组合索引使用最左前缀。 建立索引的时机 一般来说，在 WHERE 和 JOIN 中出现的列需要建立索引，但也不完全如此。 因为 B-Tree 只对 >= BETWEEN IN 以及不以通配符开始的 LIKE 使用索引。 正确使用索引 使用索引时，有以下一些技巧和注意事项： 索引字段尽量使用数字型（简单的数据类型） 若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。 这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 尽量不要让字段的默认值为 NULL 含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。 索引不会包含有 NULL 值的列，复合索引中只要有一列含有 NULL 值，那么这一列对于此复合索引就是无效的。 在数据库设计时尽量不要让字段的默认值为 NULL，应该指定列为 NOT NULL，除非你想存储 NULL。 你应该用 0、特殊的值或者空串代替空值。 前缀索引和索引选择性 对串列进行索引，如果可能应该指定一个前缀长度。 对于 BLOB、TEXT 或者很长的 VARCHAR 类型的列，必须使用前缀索引，因为 MariaDB、MySQL 不允许索引这些列的完整长度。 前缀索引是一种能使索引更小、更快的有效办法，但另一方面也有其缺点。 MariaDB、MySQL 无法使用前缀索引做 order by 和 group by，也无法使用前缀索引做覆盖扫描。 一般情况下某个前缀的选择性也是足够高的，足以满足查询性能。 例如，如果有一个 CHAR(255) 的列，如果在前 10 个或 20 个字符内，多数值是惟一的，那么就不要对整个列进行索引。 短索引不仅可以提高查询速度而且可以节省磁盘空间和 I/O 操作。 绝大多数数据库中的字符串数据大都以各种各样的名字为主，把索引的长度设置为10~15个字符已经足以把搜索范围缩小到很少的几条数据记录了。 通常可以索引开始的部分字符，这样可以大大节约索引空间，从而提高索引效率。但这样也会降低索引的选择性。 索引的选择性是指，不重复的索引值（基数）和数据表中的记录总数的比值。 索引的选择性越高则查询效率越高，因为选择性高的索引可以让MYSQL在查找时过滤掉更多的行。 唯一索引的选择性是 1，这是最好的索引选择性，性能也是最好的。 决窍在于要选择足够长的前缀以保证较高的选择性，同时又不能太长，以便节约空间。 前缀应该足够长，以使得前缀索引的选择性接近于索引整个列。 换句话说，前缀的基数应该接近于完整列的基数。 使用唯一索引 考虑某列中值的分布。索引的列的基数越大，索引的效果越好。 例如，存放出生日期的列具有不同值，很容易区分各行。 而用来记录性别的列，只含有 M 和 F，则对此列进行索引没有多大用处，因为不管搜索哪个值，都会得出大约一半的行。 使用组合索引代替多个列索引 一个组合索引与多个列索引的解析执行是不一样的，如果在 explain 中看到有索引合并，应该好好检查一下查询的表和结构是不是已经最优。 重复、冗余、不使用的索引 MariaDB、MySQL 允许在相同的列上创建多个索引，无论是有意还是无意的。大多数情况下不需要使用冗余索引。 对于重复、冗余、不使用的索引可以直接删除这些索引。因为这些索引需要占用物理空间，并且也会影响更新表的性能。 如果对大的文本进行搜索，使用全文索引而不要用使用 like ‘%…%’ like 语句不要以通配符开头 对于 like，在以通配符 % 和 _ 开头作查询时，MariaDB、MySQL 不会使用索引。 like 操作一般在全文索引中会用到，当然，InnoDB 不支持全文索引。 这句会使用索引：SELECT * FROM mytable WHERE username like'admin%' ，而这句就不会使用：SELECT * FROM mytable WHEREt Name like'%admin' 不要在列上进行运算 索引列不能是表达式的一部分，也不是是函数的参数。例如以下两个查询无法使用索引： 表达式：select actor_id from sakila.actor where actor_id+1=5; 函数参数：select ... where TO_DAYS(CURRENT_DATE) - TO_DAYS(date_col) 尽量不要使用 NOT IN、<>、!= 操作 应尽量避免在 where 子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描。 对于 not in，可以用 not exists 或者外联结加判断为空来代替。 很多时候用 exists 代替 in 是一个好的选择: select num from a where num in(select num from b) select num from a where exists(select 1 from b where num=a.num) 对于 <>，用其它相同功能的操作运算代替，如 a<>0 改为 a>0 or a or 条件 用 or 分割开的条件，如果 or 前的条件中的列有索引，而后面的列中没有索引，那么涉及到的索引都不会被用到。 应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num=10 or num=20 select id from t where num=10 union all select id from t where num=20 组合索引的使用要遵守最左前缀原则CREATE TABLE People ( last_name varchar(50) not null, first_name varchar(50) not null, birthday date not null, gender enum(‘m', 'f') not null, key(1ast_name, first_name, birthday) ); 当不需要考虑排序和分组时，将选择性最高的列放在前面通常是最好的。 查询必须从索引的最左边的列开始，否则无法使用索引。例如，你不能直接利用索引查找在某一天出生的人。 不能跳过某一索引列。例如，你不能利用索引查找 last name 为 Smith 且出生于某一天的人。 存储引擎不能使用索引中范围条件右边的列。例如，如果你的查询语句为 WHERE last_name=\"Smith\" AND first_name LIKE 'J%' AND dob='1976-12-23' ，则该查询只会使用索引中的前两列，因为 LIKE 是范围查询。 使用索引排序时，ORDER BY 也要遵守最左前缀原则 当索引的顺序与 ORDER BY 中的列顺序相同，且所有的列是同一方向（全部升序或者全部降序）时，可以使用索引来排序。 ORDER BY 子句和查询型子句的限制是一样的，需要满足索引的最左前缀的要求。 有一种情况下 ORDER BY 子句可以不满足索引的最左前缀要求，那就是前导列为常量时，WHERE 或者 JOIN 子句中对前导列指定了常量。 如果查询是连接多个表，仅当 ORDER BY 中的所有列都是第一个表的列时才会使用索引。其它情况都会使用 filesort 文件排序。 如果列类型是字符串，那么一定记得在 where 条件中把字符常量值用引号引起来，否则的话即便有索引也不会用到的。 因为 MariaDB、MySQL 默认把输入的常量值进行转换以后才进行检索。 任何地方都不要使用 select * from t ，用具体的字段列表代替 *，不要返回用不到的任何字段。 如果 MariaDB、MySQL 估计使用索引比全表扫描更慢，则不使用索引。 当索引列有大量数据重复时，查询可能不会去利用索引，如表中有字段性别，男女几乎各一半，即使在建了索引也对查询效率起不了作用。 性能测试与优化 只有当数据库里已经有了足够多的测试数据时，它的性能测试结果才有实际参考价值。 如果在测试数据库里只有几百条数据记录，它们往往在执行完第一条查询命令之后就被全部加载到内存了。 这将使后续的查询命令都执行得非常快，不管有没有使用索引。 只有当数据库里的记录超过了 1000 条、数据总量也超过了服务器上的内存总量时，数据库的性能测试结果才有意义。 在不确定应该在哪些数据列上创建索引的时候，从 EXPLAIN SELECT 命令那里往往可以获得一些帮助。 这其实只是简单地给一条普通的 SELECT 命令加一个 EXPLAIN 关键字作为前缀而已。 有了这个关键字，数据库将不是去执行那条 SELECT 命令，而是去对它进行分析， 以表格的形式把查询的执行过程和用到的索引等信息列出来，如果有的话。 查看索引使用情况：show status like 'Handler_read%' 如果索引正在工作，Handler_read_key 的值将很高。 这个值代表了一个行被索引值读的次数，很低的值表明增加索引得到的性能改善不高，因为索引并不经常使用。 Handler_read_rnd_next 的值高则意味着查询运行低效，并且应该建立索引补救。 这个值的含义是在数据文件中读下一行的请求数。 如果正进行大量的表扫描， Handler_read_rnd_next 的值较高，则通常说明表索引不正确或写入的查询没有利用索引。 优缺点 优点 索引减小了服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机IO变成顺序IO 索引对于 InnoDB 非常重要，因为它可以让查询锁更少的元组 InnoDB 在二级索引上使用共享锁，但访问主键索引需要排他锁 缺点 虽然索引提高了查询速度，同时却会降低更新速度，因为不仅要保存数据，还要保存索引文件 建立索引会占用磁盘空间的索引文件。如果在一个大表上创建了多种组合索引，索引文件的会膨胀很快 如果某个数据列包含许多重复的内容，为它建立索引就没有太大的实际效果 对于非常小的表，大部分情况下简单的全表扫描更高效 索引只是提高效率的一个因素，如果有大数据量的表，就需要花时间研究建立最优秀的索引或优化查询语句。 因此应该只为最经常查询和最经常排序的数据列建立索引。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/2.4.优化.html":{"url":"数据库/2.4.优化.html","title":"优化","keywords":"","body":"优化 SQL 语句优化 不要使用 select * 减少子查询，使用关联查询 left join right join inner join 替代 减少使用 IN NOT IN，使用 exists not exists 或者关联查询语句替代 or 尽量用 union 或者 union all 代替，在确认没有重复数据或者不用剔除重复数据时 union all 会更好 避免在 where 中使用 != 或 <> 和对字段进行 null 值判断，否则将引擎放弃使用索引而进行全表扫描 不做列运算，任何对列的操作都将导致表扫描 语句尽可能简单，一条语句只能在一个 CPU 运算，拆分语句以减少锁时间，一条大语句搞不好可以堵死整个库 不用函数和触发器，在程序内实现 列表数据不要拿全表，要使用 LIMIT 来分页，每页数量也不要太大 使用同类型进行比较 对于连续数值，使用 BETWEEN 不用 IN，SELECT id FROM t WHERE num BETWEEN 1 AND 5 可通过开启慢查询日志来找出较慢的 SQL 读写分离 经典的数据库拆分方案，主库负责写，从库负责读。 一般不要采用双主或多主引入很多复杂性，尽量采用其他方案来提高性能。 同时目前很多拆分的解决方案同时也兼顾考虑了读写分离。 字段设计 尽量使用 TINYINT、SMALLINT、MEDIUM_INT 作为整数类型而非 INT，非负则加上 UNSIGNED VARCHAR 的长度只分配真正需要的空间 使用枚举或整数代替字符串类型 尽量使用 TIMESTAMP 而非 DATETIME 单表不要有太多字段，建议在 20 以内 避免使用 NULL 字段，很难查询优化且占用额外索引空间 用整型来存 IP 索引 缓存 MariaDB 内部：系统调优参数 数据访问层：比如 MyBatis 针对 SQL 语句做缓存，Hibernate 可以精确到单个记录，这里缓存的对象主要是持久化对象 应用服务层：这里可以通过编程手段对缓存做到更精准的控制和更多的实现策略，这里缓存的对象是数据传输对象 Web 层：针对 Web 页面做缓存 浏览器客户端：用户端的缓存 可以根据实际情况在一个层次或多个层次结合加入缓存。 这里重点介绍下服务层的缓存实现，目前主要有两种方式： 直写式：在数据写入数据库后，同时更新缓存，维持数据库与缓存的一致性。 这也是当前大多数应用缓存框架的工作方式。这种实现非常简单，同步好，但效率一般。 回写式：当有数据要写入数据库时，只会更新缓存，然后异步批量的将缓存数据同步到数据库上。 这种实现比较复杂，需要较多的应用逻辑，同时可能会产生数据库与缓存的不同步，但效率非常高。 分区 在 扩展 里提到的垂直扩展、水平扩展。 垂直扩展根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。 简单来说是指数据表列的拆分，把一张列比较多的表拆分为多张表。 可以使得列数据变小，在查询时减少读取的 Block 数，减少 I/O，简化表的结构，易于维护。 但主键会出现冗余，需要管理冗余列，并会引起 Join 操作，让事务变得更加复杂。 水平扩展是指数据表行的拆分，把一张的表的数据拆成多张表来存放。 例如，将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。 水平扩展可以支持非常大的数据量，但分表仅解决了单一表数据过大的问题，表的数据还是在同一台机器上，对并发能力没有什么提升，最好分库。 水平扩展支持非常大的数据量存储，应用端改造也少，但分片事务难以解决，跨节点 Join 性能较差，逻辑复杂。 尽量不要对数据进行分片，因为拆分会提升逻辑、部署、运维的复杂度。 如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络 I/O。 分片原则 能不分就不分 分片数量尽量少，分片尽量均匀分布在多个数据结点上 因为一个查询 SQL 跨分片越多，则总体性能越差，虽然要好于所有数据在一个分片的结果，只在必要的时候进行扩容，增加分片数量。 分片规则需要慎重选择做好提前规划 分片规则的选择，需要考虑数据的增长模式，数据的访问模式，分片关联性问题，以及分片扩容问题。 分片策略为范围分片，枚举分片，一致性 Hash 分片，这几种分片都有利于扩容。 尽量不要在一个事务中的 SQL 跨越多个分片，分布式事务一直是个不好处理的问题 查询条件尽量优化 尽量避免大结果集，这会消耗大量带宽和 CPU 资源，尽量为频繁使用的查询语句建立索引。 通过数据冗余和表分区赖降低跨 Join 的可能 如果某个表的数据有明显的时间特征，比如订单、交易记录等，则他们通常比较合适用时间范围分片。 因为具有时效性的数据，往往关注其近期的数据，查询条件中往往带有时间字段进行过滤。 比较好的方案是，当前活跃的数据，采用跨度比较短的时间段进行分片，而历史性的数据，则采用比较长的跨度存储。 总体上来说，分片的选择是取决于最频繁的查询 SQL 的条件，因为不带任何 Where 语句的查询 SQL，会遍历所有的分片，性能相对最差。 这种 SQL 越多，对系统的影响越大，要尽量避免这种 SQL 的产生。 客户端架构 分片逻辑在应用端，例如 Tumblr 的 JetPants。 优点： 应用直连数据库，降低外围系统依赖所带来的宕机风险 集成成本低，无需额外运维的组件 缺点： 限于只能在数据库访问层上做文章，扩展性一般，对于比较复杂的系统可能会力不从心 将分片逻辑的压力放在应用服务器上，造成额外风险 中间件架构 分片逻辑在中间件服务中，在应用和数据中间加了一个代理层，例如 Youtube 的 Vitess。 优点： 能够处理非常复杂的需求，不受数据库访问层原来实现的限制，扩展性强 对于应用服务器透明且没有增加任何额外负载 缺点： 需部署和运维独立的代理中间件，成本高 应用需经过代理来连接数据库，网络上多了一跳，性能有损失且有额外风险 方案选择 确定架构，中小型规模或是比较简单的场景倾向于选择客户端架构，复杂场景或大规模系统倾向选择中间件架构 功能是否满足，比如需要跨节点 ORDER BY，那么支持该功能的优先考虑。 活跃度，不考虑开发停滞、无人维护的项目 开源优先，因为可能会有特殊需求要改动源码 系统调优参数 具体的调优参数内容较多，建议直接参考官方文档。 back_log 在暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中 如果连接数达到 max_connections，新请求将会被存在堆栈中以等待某一连接释放资源。 如果等待连接的数量超过 back_log，新请求会被拒绝。可以从默认的 50 升至 500。 wait_timeout 数据库连接闲置时间 闲置连接会占用内存资源，可以从默认的 8 小时减到半小时。 max_user_connection 最大连接数 默认为0无上限，最好设一个合理上限。 thread_concurrency 并发线程数 设为 CPU 核数的两倍。 skip_name_resolve 禁止对外部连接进行 DNS 解析 可以消除 DNS 解析时间，但需要所有远程主机用 IP 访问。 key_buffer_size 索引块的缓存大小 增加会提升索引处理速度，对 MyISAM 表性能影响大。 对于内存 4G 左右，可设为 256M 或 384M。 通过查询 show status like 'key_read%'，保证 key_reads / key_read_requests 在 0.1% 以下最好。 innodb_buffer_pool_size 缓存数据块和索引块 对 InnoDB 表性能影响大。 通过查询 show status like 'Innodb_buffer_pool_read%' 保证 (Innodb_buffer_pool_read_requests – Innodb_buffer_pool_reads) / Innodb_buffer_pool_read_requests 越高越好。 innodb_additional_mem_pool_size InnoDB 存储引擎用来存放数据字典信息以及一些内部数据结构的内存空间大小 当数据库对象非常多的时候，适当调整该参数的大小以确保所有数据都能存放在内存中提高访问效率。 当过小的时候，数据库会记录 Warning 信息到数据库的错误日志中，这时就需要该调整这个参数大小。 innodb_log_buffer_size InnoDB 存储引擎的事务日志所使用的缓冲区 一般来说不建议超过 32MB。 query_cache_size 缓存 MariaDB 中的 ResultSet 就是一条SQL语句执行的结果集，只能针对 select 语句。 当某个表的数据有任何任何变化，都会导致所有引用了该表的 select 语句在 Query Cache 中的缓存数据失效。 当数据变化非常频繁的情况下，使用 Query Cache 可能会得不偿失。 根据命中率 (Qcache_hits/(Qcache_hits+Qcache_inserts)*100)) 进行调整，一般 256MB 差不多了，大型的配置型静态数据可适当调大。 可以通过命令 show status like 'Qcache_%' 查看目前系统 Query catch 使用大小。 read_buffer_size 读入缓冲区大小 对表进行顺序扫描的请求将分配一个读入缓冲区，会为它分配一段内存缓冲区。 如果对表的顺序扫描请求非常频繁，可以通过增加该变量值以及内存缓冲区大小提高其性能。 sort_buffer_size 执行排序使用的缓冲大小 如果想要增加 ORDER BY 的速度，首先看是否可以让 MariaDB 使用索引而不是额外的排序阶段。 如果不能，可以尝试增加变量的大小。 read_rnd_buffer_size 随机读缓冲区大小 当按任意顺序读取行时，将分配一个随机读缓存区。 进行排序查询时，会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，如果需要排序大量数据，可适当调高该值。 但 MariaDB 会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大。 record_buffer 顺序扫描缓冲区 每个进行一个顺序扫描的线程为其扫描的每张表分配这个大小的一个缓冲区。如果有很多顺序扫描，可以增加该值。 thread_cache_size 缓存连接服务的线程 保存当前没有与连接关联但是准备为后面新的连接服务的线程，可以快速响应连接的线程请求而无需创建新的。 table_cache 缓存表文件 类似于 thread_cache_size，但用来缓存表文件，对 InnoDB 效果不大，主要用于 MyISAM。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/2.5.锁机制.html":{"url":"数据库/2.5.锁机制.html","title":"锁机制","keywords":"","body":"锁机制 锁是计算机协调多个进程或线程并发访问某一资源的机制。 锁保证数据并发访问的一致性、有效性；锁冲突也是影响数据库并发访问性能的一个重要因素。 锁是 MariaDB 在服务器层和存储引擎层的的并发控制。 加锁是消耗资源的，锁的各种操作，包括获得锁、检测锁是否是否已解除、释放锁等。 共享锁与排他锁 共享锁（读锁）：其他事务可以读，但不能写。 排他锁（写锁） ：其他事务不能读取，也不能写。 粒度锁 不同的存储引擎支持不同的锁机制，所有的存储引擎都以自己的方式显现了锁机制，服务器层完全不了解存储引擎中的锁实现： MyISAM 和 MEMORY 存储引擎采用的是表锁 BDB 存储引擎采用的是页面锁，但也支持表锁 InnoDB 存储引擎既支持行锁，也支持表锁，但默认情况下是采用行级锁 默认情况下，表锁和行锁都是自动获得的， 不需要额外的命令。 但是在有的情况下，用户需要明确地进行锁表或者进行事务的控制，以便确保整个事务的完整性，这样就需要使用事务控制和锁定语句来完成。 不同粒度锁的比较： 表锁 行锁 页面锁 开销 小️ 大 中等 速度 快 慢 中等 死锁 无 有 有 锁定粒度 大 小 中等 锁冲突概率 高 低 中等 并发度 低 高 中等 表级锁 开销小，加锁快，不会出现死锁，锁定粒度大，发生锁冲突的概率最高，并发度最低。 这些存储引擎通过总是一次性同时获取所有需要的锁以及总是按相同的顺序获取表锁来避免死锁。 表级锁更适合于以查询为主，并发用户少，只有少量按索引条件更新数据的应用。 行级锁 开销大，加锁慢，会出现死锁，锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 最大程度的支持并发，同时也带来了最大的锁开销。 在 InnoDB 中，除单个 SQL 组成的事务外，锁是逐步获得的，这就决定了在 InnoDB 中发生死锁是可能的。 行级锁只在存储引擎层实现，而服务器层没有实现。 行级锁更适合于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用 页面锁 开销和加锁时间界于表锁和行锁之间，会出现死锁，锁定粒度界于表锁和行锁之间，并发度一般。 MyISAM 表锁 表共享读锁：不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求 表独占写锁：会阻塞其他用户对同一表的读和写操作 MyISAM 表的读操作与写操作之间，以及写操作之间是串行的。 当一个线程获得对一个表的写锁后，只有持有锁的线程可以对表进行更新操作。 其他线程的读写操作都会等待，直到锁被释放为止。 默认情况下，写锁比读锁具有更高的优先级。 当一个锁释放时，这个锁会优先给写锁队列中等候的获取锁请求，然后再给读锁队列中等候的获取锁请求。 这也正是 MyISAM 表不太适合于有大量更新操作和查询操作应用的原因。 因为，大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞。 同时，一些需要长时间运行的查询操作，也会使写线程堵塞，应用中应尽量避免出现长时间运行的查询操作。 在可能的情况下可以通过使用中间表等措施对 SQL 语句做一定程度的分解，使每一步查询都能在较短时间完成，从而减少锁冲突。 如果复杂查询不可避免，应尽量安排在数据库空闲时段执行，比如一些定期统计可以安排在夜间执行。 可以设置改变读锁和写锁的优先级： 通过指定启动参数 low-priority-updates 使 MyISAM 引擎默认给予读请求以优先的权利 通过执行命令 SET LOW_PRIORITY_UPDATES=1 使该连接发出的更新请求优先级降低 通过指定 INSERT UPDATE DELETE 语句的 LOW_PRIORITY 属性降低该语句的优先级 给系统参数 max_write_lock_count 设置一个合适的值，当读锁达到这个值后会将写请求的优先级降低，给读进程一定的机会 MyISAM 加表锁方法： MyISAM 在 SELECT 前会自动给涉及的表加读锁 在执行更新操作 UPDATE DELETE INSERT 等前会自动给涉及的表加写锁 这个过程并不需要用户干预，因此，用户一般不需要直接用 LOCK TABLE 命令给 MyISAM 表显式加锁。 在自动加锁的情况下，MyISAM 总是一次获得 SQL 语句所需要的全部锁，这也正是 MyISAM 表不会出现死锁的原因。 MyISAM 存储引擎支持并发插入，以减少给定表的读和写操作之间的争用： 如果 MyISAM 表在数据文件中间没有空闲块，则行始终插入数据文件的末尾。 在这种情况下可以自由混合并发使用 MyISAM 表的 INSERT SELECT 语句而不需要加锁。 你可以在其他线程进行读操作的时候，同时将行插入到 MyISAM 表中。 文件中间的空闲快可能是从表格中间删除或更新的行而产生的。 如果文件中间有空闲快，则并发插入会被禁用，但是当所有空闲块都填充有新数据时，它又会自动重新启用。 要控制此行为，可以使用 concurrent_insert 变量。 如果使用 LOCK TABLES 显式获取表锁，则可以请求 READ LOCAL 锁而不是 READ 锁，以便在锁定表时，其他会话可以使用并发插入。 concurrent_insert 为 0 时不允许并发插入 concurrent_insert 为 1 且表中没有空洞时，允许在一个线程读的同时，另一个线程从表尾插入 concurrent_insert 为 2 时，无论表中有没有空洞，都允许在表尾并发插入记录 查询表级锁争用情况： 可以通过检查 table_locks_waited 和 table_locks_immediate 状态变量来分析系统上的表锁的争夺 如果 Table_locks_waited 的值比较高，则说明存在着较严重的表级锁争用情况。 总结 MyISAM 引擎支持表锁 表级锁分为两种：共享读锁、互斥写锁，这两种锁都是阻塞锁 可以在读锁上增加读锁，不能在读锁上增加写锁，在写锁上不能增加写锁 默认情况下，M执行查询语句之前会加读锁，在执行更新语句之前会执行写锁 如果想要显示的加锁/解锁的花可以使用 LOCK TABLES 和 UNLOCK 在使用 LOCK TABLES 之后，在解锁之前不能操作未加锁的表 在加锁时指明是要增加读锁，那么在解锁之前只能进行读操作，不能执行写操作 如果一次语句要操作的表以别名的方式多次出现，那么就要在加锁时都指明要加锁的表的别名 concurrent_insert 专门用以控制其并发插入的行为，其值分别可以为 0、1、2 由于读写锁互斥，在调度过程中，写锁优先的原则。可以通过 low-priority-updates 设置 InnoDB 行锁和表锁 共享锁：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁 排他锁：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁 意向共享锁：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的意向共享锁 意向排他锁：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的意向排他锁 锁模式的兼容情况 共享锁 排他锁 意向共享锁 意向排他锁 共享锁 兼容 冲突 兼容 冲突 排他锁 冲突 冲突 冲突 冲突 意向共享锁 兼容 冲突 兼容 兼容 意向排他锁 冲突 冲突 兼容 兼容 如果一个事务请求的锁模式与当前的锁兼容， InnoDB 就将请求的锁授予该事务；反之，该事务就要等待锁释放。 加锁方法 意向锁是自动加的，不需用户干预 对于 UPDATE DELETE INSERT 语句会自动给涉及数据集加排他锁 对于普通 SELECT 语句不会加任何锁 事务可以通过以下语句显式给记录集加共享锁或排他锁： 共享锁：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE。 其他会话可以查并也可以对该记录加 share mode 的共享锁，但是如果当前事务需要对该记录进行更新操作，则很有可能造成死锁。 排他锁：SELECT * FROM table_name WHERE ... FOR UPDATE。 其他会话可以查询该记录，但是不能对该记录加共享锁或排他锁，而是等待获得锁。 隐式锁定： InnoDB 在事务执行过程中，使用两阶段锁协议： 随时都可以执行锁定，InnoDB 会根据隔离级别在需要的时候自动加锁， 锁只有在执行 commit 或者 rollback 的时候才会释放，并且所有的锁都是在同一时刻被释放。 显式锁定 ： 共享锁：SELECT ... LOCK IN SHARE MODE。 一般情况下，为了确保自己查到的数据没有被其他的事务正在修改，也就是说确保查到的数据是最新的数据，并且不允许其他人来修改数据。 但是自己不一定能够修改数据，因为有可能其他的事务也对这些数据，使用这种方式上共享锁。 作用就是将查找到的数据加上一个共享锁，这个就是表示其他的事务只能对这些数据进行简单的 select 操作，并不能够进行 DML 操作。 排他锁：SELECT ... FOR UPDATE。 一般情况下，为了让自己查到的数据确保是最新数据，并且查到后的数据只允许自己来修改的时候，需要用到这个语句。 在执行这个查询语句的时候，会将对应的索引访问条目进行上排他锁，也就是说这个语句对应的锁就相当于 update 带来的效果。 显示锁定在业务繁忙的情况下，如果不能及时 commit 或者 rollback 可能会造成其他事务长时间的等待，影响并发使用效率。 区别在于，前一个是共享锁，多个事务可以同时的对相同数据执行， 后一个上的是排他锁，一旦一个事务获取了这个锁，其他的事务是没法在这些数据上执行 FOR UPDATE。 行锁实现方式 InnoDB 行锁是通过给索引上的索引项加锁来实现的，只有通过索引条件检索数据，才使用行级锁，否则使用表锁. Oracle 是通过在数据块中对相应数据行加锁来实现的。 不论是使用主键索引、唯一索引或普通索引，InnoDB 都会使用行锁来对数据加锁。 只有执行计划真正使用了索引，才能使用行锁。 即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的。 如果 MySQL 认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。 在分析锁冲突时，别忘了检查 SQL 的执行计划以确认是否真正使用了索引。 由于 MySQL 的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然多个会话是访问不同行的记录， 但是如果是使用相同的索引键，是会出现锁冲突的，后使用这些索引的会话需要等待先使用索引的会话释放锁后才能获取锁。 间隙锁 当用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB 会给符合条件的已有数据记录的索引项加锁。 对于键值在条件范围内但并不存在的记录，叫做间隙（GAP)，InnoDB 也会对这个间隙加锁，这种锁机制就是所谓的间隙锁。 在使用范围条件检索并锁定记录时，InnoDB 这种加锁机制会阻塞符合条件范围内键值的并发插入，这往往会造成严重的锁等待。 在实际应用开发中，尤其是并发插入比较多的应用，要尽量优化业务逻辑，尽量使用相等条件来访问更新数据，避免使用范围条件。 使用间隙锁可以防止幻读，以满足相关隔离级别的要求，还能满足恢复和复制的需要： MySQL 通过 Binlog 录入执行成功的 INSERT、UPDATE、DELETE 等更新数据的 SQL 语句，并由此实现 MySQL 数据库的恢复和主从复制。 MySQL 的恢复机制（复制其实就是在 Slave MySQL 不断做基于 Binlog 的恢复）有以下特点： MySQL 的恢复是 SQL 语句级的，也就是重新执行 Binlog 中的 SQL 语句 MySQL 的 Binlog 是按照事务提交的先后顺序记录的，恢复也是按这个顺序进行的 MySQL 的恢复机制要求：在一个事务未提交前，其他并发事务不能插入满足其锁定条件的任何记录，也就是不允许出现幻读。 事务隔离性与锁 一般来说，直接操作数据库中各种锁的几率相对比较少，更多的是利用数据库提供的四个隔离级别。 隔离级别 脏读 丢失更新 不可重复读 幻读 未提交读 ✔️ ✔️ ✔️ ✔️ 已提交读 ❌ ❌ ✔️ ✔️ 可重复读 ❌ ❌ ❌ ✔️ 可串行读 ❌ ❌ ❌ ❌ 隔离级别越高，锁就越严格，尤其是使用范围条件的时候，产生锁冲突的可能性也就越高，从而对并发性事务处理性能的影响也就越大。 在应用中应该尽量使用较低的隔离级别，以减少锁争用的机率。 实际上，通过优化事务逻辑，大部分应用使用 Read Commited 隔离级别就足够了。 对于一些确实需要更高隔离级别的事务，可以通过在程序中动态改变隔离级别的方式满足需求。 SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE 获取行锁争用情况 可以通过检查 InnoDB_row_lock 状态变量来分析系统上的行锁的争夺情况。 show status like 'innodb_row_lock%'; LOCK TABLES 和 UNLOCK TABLES LOCK TABLES 和 UNLOCK TABLES 都是在服务器层实现的，和存储引擎无关，它们有自己的用途，并不能替代事务处理。 除了禁用了 autocommint 后可以使用，其他情况不建议使用。 LOCK TABLES 可以锁定用于当前线程的表。如果表被其他线程锁定，则当前线程会等待，直到可以获取所有锁定为止。 UNLOCK TABLES 可以释放当前线程获得的任何锁定。 当前线程执行另一个 LOCK TABLES 时或当与服务器的连接被关闭时，所有由当前线程锁定的表被隐含地解锁。 使用LOCK TABLES的场景 显式加表级锁一般是为了在一定程度模拟事务操作，实现对某一时间点多个表的一致性读取。 给表显式加表锁时，必须同时取得所有涉及到表的锁。 MySQL 不支持锁升级，在执行 LOCK TABLES 后，只能访问显式加锁的这些表，不能访问未加锁的表。 同时，如果加的是读锁，那么只能执行查询操作，而不能执行更新操作。 其实，在 MyISAM 自动加锁的情况下也大致如此，MyISAM 总是一次获得 SQL 语句所需要的全部锁。 这也正是 MyISAM 表不会出现死锁的原因。 死锁 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环。 当事务试图以不同的顺序锁定资源时，就可能产生死锁。多个事务同时锁定同一个资源时也可能会产生死锁。 锁的行为和顺序和存储引擎相关。以同样的顺序执行语句，有些存储引擎会产生死锁有些不会。 死锁原因 真正的数据冲突和存储引擎的实现方式。 检测死锁 数据库系统实现了各种死锁检测和死锁超时的机制。 InnoDB 存储引擎能检测到死锁的循环依赖并立即返回一个错误。 死锁恢复 死锁发生以后，只有部分或完全回滚其中一个事务，才能打破死锁。 InnoDB 目前处理死锁的方法是，将持有最少行级排他锁的事务进行回滚。 所以事务型应用程序在设计时必须考虑如何处理死锁，多数情况下只需要重新执行因死锁回滚的事务即可。 外部锁的死锁检测 发生死锁后，InnoDB 一般都能自动检测到，并使一个事务释放锁并回退，另一个事务获得锁，继续完成事务。 但在涉及外部锁或涉及表锁的情况下，InnoDB 并不能完全自动检测到死锁。 这需要通过设置锁等待超时参数 innodb_lock_wait_timeout 来解决。 影响性能影响 死锁会影响性能而不是会产生严重错误，因为 InnoDB 会自动检测死锁状况并回滚其中一个受影响的事务。 在高并发系统上，当许多线程等待同一个锁时，死锁检测可能导致速度变慢。 有时当发生死锁时，禁用死锁检测 innodb_deadlock_detect 可能会更有效。 这时可以依赖 innodb_lock_wait_timeout 设置进行事务回滚。 避免死锁 在自动加锁的情况下，MyISAM 总是一次获得 SQL 语句所需要的全部锁，所以 MyISAM 表不会出现死锁。 为了在单个 InnoDB 表上执行多个并发写入操作时避免死锁， 可以在事务开始时通过为预期要修改的每个元祖使用 SELECT ... FOR UPDATE 语句来获取必要的锁， 即使这些行的更改语句是在之后才执行的。 在事务中，如果要更新记录，应该直接申请足够级别的锁，即排他锁，而不应先申请共享锁、更新时再申请排他锁， 因为这时候当用户再申请排他锁时，其他事务可能又已经获得了相同记录的共享锁，从而造成锁冲突，甚至死锁。 如果事务需要修改或锁定多个表，则应在每个事务中以相同的顺序使用加锁语句。 在应用中，如果不同的程序会并发存取多个表，应尽量约定以相同的顺序来访问表，这样可以降低产生死锁的机会。 通过 SELECT ... LOCK IN SHARE MODE 获取行的写锁后，如果当前事务再需要对该记录进行更新操作，则很有可能造成死锁。 改变事务隔离级别 如果出现死锁，可以用 SHOW INNODB STATUS 确定最后一个死锁产生的原因。 返回结果中包括死锁相关事务的详细信息，如引发死锁的 SQL 语句，事务已经获得的锁，正在等待什么锁，以及被回滚的事务等。 据此可以分析死锁产生的原因和改进措施。 乐观、悲观锁 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性，不能解决脏读的问题。 乐观锁每次去拿数据的时候都认为别人不会修改，所以不会上锁。 在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。 乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于 write_condition 机制的其实都是提供的乐观锁。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。 悲观锁每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁。 传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 优化锁性能 尽量使用较低的隔离级别； 精心设计索引，并尽量使用索引访问数据，使加锁更精确 选择合理的事务大小，小事务发生锁冲突的几率也更小 给记录集显示加锁时，最好一次性请求足够级别的锁 不同的程序访问一组表时，应尽量约定以相同的顺序访问各表，对一个表而言，尽可能以固定的顺序存取表中的行 尽量用相等条件访问数据，这样可以避免间隙锁对并发插入的影响 不要申请超过实际需要的锁级别 除非必须，查询时不要显示加锁。 MySQL 的 MVCC 可以实现事务中的查询不用加锁，优化事务性能 对于一些特定的事务，可以使用表锁来提高处理速度或减少死锁的可能 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/2.6.数据类型.html":{"url":"数据库/2.6.数据类型.html","title":"数据类型","keywords":"","body":"数据类型 数值型 名称 类型和描述 存储空间(Byte) 值范围（有符号） 值范围（无符号） TINYINT 小整数值 1 -127 ~ 127 0 ~ 255 SMALLINT 大整数值 2 -32,768 ~ 32,767 0 ~ 65,535 MEDIUMINT 大整数值 3 -8,388,608 ~ 8,388,607 0 ~ 16,777,215 INT 大整数值 4 -2,147,483,648 ~ 2,147,483,647 0 ~ 4,294,967,295 INTEGER 大整数值 4 -2,147,483,648 ~ 2,147,483,647 0 ~ 4,294,967,295 BIGINT 极大整数值 8 -9,223,372,036,854,775,808 ~ 9,223,372,036,854,775,807 0 ~ 18,446,744,073,709,551,615 FLOAT 单精度浮点数值 4 IEEE754 IEEE754 DOUBLE 双精度浮点数值 8 IEEE754 IEEE754 DECIMAL 定点型 16 - - 任何整数类型都可以加上 UNSIGNED 属性，表示数据是无符号的，即非负整数。 整数类型可以被指定长度，例如：INT(11) 表示长度为 11 的 INT 类型。 不过，长度在大多数场景是没有意义的，它不会限制值的合法范围，只会影响显示字符的个数。 而且需要和 UNSIGNED ZEROFILL 属性配合使用才有意义。 假定类型设定为 INT(5)，属性为 UNSIGNED ZEROFILL，如果用户插入的数据为 12 的话，那么数据库实际存储数据为 00012。 DECIMAL 可以用于存储比 BIGINT 还大的整型，能存储精确的小数。 而 FLOAT 和 DOUBLE 是有取值范围的，并支持使用标准的浮点进行近似计算。 计算时 FLOAT 和 DOUBLE 相比 DECIMAL 效率更高一些。 日期和时间型 名称 类型和描述 存储空间(Byte) 范围 DATE 日期 3 1000-01-01 ~ 9999-12-31 TIME 时间或持续时间 3 -838:59:59 ~ 838:59:59 YEAR 年份 1 1901 ~ 2155 DATETIME 混合日期时间 8 1000-01-01 00:00:00 ~ 9999-12-31 23:59:59 TIMESTAMP 混合日期时间 4 1970-01-01 00:00:00 ~ 2038-01-19 03:14:07 尽量使用 TIMESTAMP，空间效率高于 DATETIME，用整数保存时间戳通常不方便处理。 如果需要存储微秒，可以使用 BIGINT 存储。 字符串类型 名称 类型和描述 存储空间(Byte) CHAR 定长字符串 0 ~ 255 VARCHAR 变长字符串 0 ~ 65,535 TINYTEXT 字符串 0 ~ 255 TEXT 字符串 0 ~ 65,535 MEDIUMTEXT 字符串 0 ~ 16,777,215 LONGTEXT 字符串 0 ~ 4,294,967,295 TINYBLOB 二进制字符串 0 ~ 255 BLOB 二进制字符串 0 ~ 65,535 MEDIUMBLOB 二进制字符串 0 ~ 16,777,215 LONGBLOB 二进制字符串 0 ~ 4,294,967,295 VARCHAR 用于存储可变长字符串，它比定长类型更节省空间。 VARCHAR 使用额外 1 或 2 个字节存储字符串长度。列长度小于 255 字节时，使用 1 字节表示，否则使用 2 字节表示。 VARCHAR 存储的内容超出设置的长度时，内容会被截断。 CHAR 是定长的，根据定义的字符串长度分配足够的空间。 CHAR 会根据需要使用空格进行填充方便比较。 CHAR 适合存储很短的字符串，或者所有值都接近同一个长度。 CHAR 存储的内容超出设置的长度时，内容同样会被截断。 对于经常变更的数据来说，CHAR 比 VARCHAR 更好，因为 CHAR 不容易产生碎片。 对于非常短的列，CHAR 比 VARCHAR 在存储空间上更有效率。 使用时要注意只分配需要的空间，更长的列排序时会消耗更多内存。 尽量避免使用 TEXT、BLOB 类型，查询时会使用临时表，导致严重的性能开销。 枚举 create table my_enum( gender enum('男', '女', '保密') ); 有时可以使用 ENUM 代替常用的字符串类型。 枚举元素的顺序会按照元素出现的顺序，从 1 开始编号。 枚举在进行数据规范的时候，系统会自动建立一个数字与枚举元素的对应关系。 在进行数据插入时，系统自动将字符转换成对应的数字存储，然后再进行数据提取的时候，系统自动将数值转换成对应的字符串显示。 因为枚举实际存储的是数值，所以可以直接插入数值。 集合 集合跟枚举很类似，实际存储的是数值，而不是字符串，但集合是多选的。 create table my_set( hobby set('篮球', '足球', '乒乓球', '羽毛球', '排球', '台球', '网球', '棒球'） ); insert into my_set values('足球， 台球， 网球'); select hobby + 0, hobby from my_set; 插入数据可使用多个元素字符串组合，也可以直接插入数值。 集合中，每一个元素都是对应一个二进制位，被选中为 1，没有则为 0，最后反过来。 集合中元素的顺序没有关系，最终系统都会去匹配顺序。 集合的强大在于能够规范数据和节省空间。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/2.7.权限.html":{"url":"数据库/2.7.权限.html","title":"权限","keywords":"","body":"权限 MariaDB 服务器通过权限表来控制用户对数据库的访问，权限表存放在数据库里，由 mysql_install_db 脚本初始化。 user 权限表：记录允许连接到服务器的用户帐号信息，里面的权限是全局级的。 db 权限表：记录各个帐号在各个数据库上的操作权限。 table_priv 权限表：记录数据表级的操作权限。 columns_priv 权限表：记录数据列级的操作权限。 host 权限表：配合db权限表对给定主机上数据库级操作权限作更细致的控制。这个权限表不受GRANT和REVOKE语句的影响。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/2.8.删除.html":{"url":"数据库/2.8.删除.html","title":"删除","keywords":"","body":"删除 数据库中 drop delete truncate 三者虽然都是删除，就是都有区别。 这当然是一句一废话 作用范围 delete 和 truncate 只删除表的数据不删除表的结构。 drop 删除数据、结构、被依赖的约束、触发器、索引，依赖于该表的存储过程和函数会保留，但状态会变为 invalid。 truncate 只能对表，delete 还可以操作 view。 truncate 删除表中数据，再插入时自增长 id 又会从 1 开始。 delete 删除表中数据，可以加 where 字句。 执行速度 一般来说: drop > truncate > delete truncate 在功能上与不带 where 子句的 delete 语句相同，均是删除表中的全部行。 但 truncate 比 delete 速度快，且使用的系统和事务日志资源少。 delete 语句每次删除一行，并在事务日志中为所删除的每行记录一项。 truncate 通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。 执行方式 delete 每次从表中删除一行，且是 dml，操作会放到 rollback segement 中，事务提交之后才生效，会激活触发器。 如果有相应的 trigger，执行的时候将被触发。 truncate 一次性删除所有数据，是 ddl，操作立即生效，原数据不放到 rollback segment 中，不能回滚，不激活触发器。 drop 是 ddl，操作立即生效，原数据不放到 rollback segment 中，不能回滚，不激活触发器。 空间 delete 操作不会减少表或索引所占用的空间。 truncate 之后，表和索引所占用的空间会恢复到初始大小。 drop 会将表所占用的空间全释放掉。 其他 对于由 FOREIGN KEY 约束引用的表，不能使用 truncate，而应使用不带 where 子句的 delete。 由于 truncate 不记录在日志中，所以不能激活触发器。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/2.9.join.html":{"url":"数据库/2.9.join.html","title":"Join","keywords":"","body":"Join join 操作分为 join、left join、right join、cross join，分别叫内联接、左外联接、右外联接、交叉联接。 内联接使用来匹配两张表中相关联的记录。 左外联接除了匹配两张表中相关联的记录外，还会匹配左表中剩余的记录，右表中未匹配到的字段用 NULL 表示。 右外联接和左外联接一样，只不过匹配的是右表中剩余的记录。 根据表名出现在 Join 的左右位置关系来判定左表右表。 交叉联接很少用到，没查资料的时候，都没想起来这个。 cross join 又称为笛卡尔乘积，实际上是把两个表乘起来。 Teacher +------------------+ | ID | TeacherName | +----|-------------+ | 1 | Mary | | 2 | Jim | -------------------+ Student +------------------------------+ | ID | TeacherID | StudentName | +----|-----------|-------------+ | 1 | 1 | Vineeth | | 2 | 1 | Unni | +------------------------------+ 如果需要找到学校中的所有教师和学生，那么交叉联接就用得上了。 SELECT Teacher.TchrId, Teacher.TeacherName, Student.StudentName FROM Teacher CROSS JOIN Student; +---------------------------------------+ | TeacherID | TeacherName | StudentName | +-----------|-------------|-------------+ | 2 | Jim | Vineeth | | 2 | Jim | Unni | | 1 | Mary | Vineeth | | 1 | Mary | Unni | +---------------------------------------+ 得吐槽一下，我要是没脑残，肯定 SELECT * from Teacher, Student 查全表。 交叉联接不能使用 on 关键字，但可以使用 where 子句定义连接条件。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"数据库/2.10.键.html":{"url":"数据库/2.10.键.html","title":"键","keywords":"","body":"键 如有两个表： 教师 +------------------+ | ID | TeacherName | +----|-------------+ | 1 | Mary | | 2 | Jim | -------------------+ 学生 +------------------------------+ | ID | TeacherID | StudentName | +----|-----------|-------------+ | 1 | 1 | Vineeth | | 2 | 1 | Unni | +------------------------------+ 超键 在关系中能唯一标识元组的属性集称为关系模式的超键。 上表中 ID 是标识学生的唯一标识，该元组的超键就是 ID。 除此之外还可以把它跟其他属性组合起来，比如：(ID, StudentName) 也是超键。 候选键 不含多余属性的超键为候选键。 根据例子可知，学号是一个可以唯一标识元组的唯一标识，因此学号是一个候选键， 实际上，候选键是超键的子集，比如 （学号，年龄）是超键，但是它不是候选键。因为它还有了额外的属性。 主键 用户选择的候选键作为该元组的唯一标识，那么它就为主键。 外键 如果关系模式 R1 中的某属性集不是 R1 的主键，而是另一个关系 R2 的主键，则该属性集是关系模式 R1 的外键。 外键是相对于主键的，比如学生表有教师表的 ID。 主键为候选键的子集，候选键为超键的子集，而外键的确定是相对于主键的。 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "},"nginx/nginx.html":{"url":"nginx/nginx.html","title":"Nginx","keywords":"","body":"简介 Nginx 是一个免费、自由、开源、高性能的 Web 服务器和反向代理服务器，用于 HTTP、HTTPS、SMTP、POP3 和 IMAP 协议。 因它的高性能、稳定性、丰富的功能、简单的配置和低系统资源的消耗而闻名。 Nginx 解决了服务器的 C10K（一秒之内连接 10k 个客户端）问题。 它的设计不像传统的服务器那样使用线程处理请求，而是事件驱动机制，是一种异步事件驱动结构。 优点 更快 得益于事件驱动机制，在正常情况下，单次请求会得到更快的响应，在高峰期（如有数以万计的并发请求）可以比其他 Web 服务器更快地响应请求。 高扩展性，跨平台 Nginx 的设计极具扩展性，它完全是由多个不同功能、不同层次、不同类型且耦合度极低的模块组成。 因此，当对某一个模块修复 Bug 或进行升级时，可以专注于模块自身，无须在意其他。 而且在 HTTP 模块中，还设计了 HTTP 过滤器模块。 一个正常的 HTTP 模块在处理完请求后，会有 HTTP 过滤器模块对请求的结果进行再处理。 当开发一个新的 HTTP 模块时，不但可以使用诸如 HTTP 核心模块，还可以复用大量已有的 HTTP 过滤器模块。 这种低耦合度的优秀设计，造就了 Nginx 庞大的第三方模块，当然，公开的第三方模块也如官方发布的模块一样容易使用。 Nginx 的模块都是嵌入到二进制文件中执行的，无论官方发布的模块还是第三方模块都是如此。 这使得第三方模块一样具备极其优秀的性能，充分利用 Nginx 的高并发特性。 因此，许多高流量的网站都倾向于开发符合自己业务特性的定制模块。 高可靠性 用于反向代理，宕机的概率微乎其微。 高可靠性是我们选择Nginx的最基本条件，因为 Nginx 的可靠性是大家有目共睹的，很多家高流量网站都在核心服务器上大规模使用 Nginx。 Nginx 的高可靠性来自于其核心框架代码的优秀设计、模块设计的简单性。 另外，官方提供的常用模块都非常稳定，每个 worker 进程相对独立，master 进程在 1 个 worker 进程出错时可以快速拉起新的 worker 子进程。 低内存消耗 一般情况下，10000 个非活跃的 HTTP Keep-Alive 连接在 Nginx 中仅消耗 2.5MB 的内存，这是 Nginx 支持高并发连接的基础。 单机支持十万并发 理论上，Nginx 支持的并发连接上限取决于内存，这并未封顶。当然，能够及时地处理更多的并发请求，是与业务特点紧密相关的。 热部署 master 管理进程与 worker 工作进程的分离设计，使得 Nginx 能够提供热部署功能。 当然，它也支持不停止服务就更新配置项、更换日志文件等功能。 BSD 许可协议 BSD 许可协议不只是允许用户免费使用，它还允许在自己的项目中直接使用或修改 Nginx 源码，然后发布。 以上7个特点当然不是Nginx的全部，拥有无数个官方功能模块、第三方功能模块使得Nginx能够满足绝大部分应用场景， 这些功能模块间可以叠加以实现更加强大、复杂的功能，有些模块还支持Nginx与Perl、Lua等脚本语言集成工作，大大提高了开发效率。 这些特点促使用户在寻找一个Web服务器时更多考虑Nginx。 选择Nginx的核心理由还是它能在支持高并发请求的同时保持高效的服务 powered by Gitbook该文件修订时间： 2020-04-10 10:05:54 "}}